[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PsyBSc8 Diagnostik - R Skript",
    "section": "",
    "text": "Einleitung\nThis is a Quarto book."
  },
  {
    "objectID": "script_itemanalysis.html#einleitung",
    "href": "script_itemanalysis.html#einleitung",
    "title": "1  Itemanalyse",
    "section": "1.1 Einleitung",
    "text": "1.1 Einleitung\nIm Rahmen dieses Tutorials zur Itemanalyse verwenden wir echte Daten aus einem im vergangenen Semester konstruierten Fragebogen zu Umweltbewusstsein, welcher die Einstellung von Personen gegenüber dem Umweltschutz misst. Spezifisch wurde dieses Konstrukt auf Verhaltensebene operationalisiert, wobei mehr oder weniger umweltschützende Verhaltensweisen als Indikator für das Ausmaß des umweltbewussten Handelns betrachtet werden. Neben den 29 Items der Rohfassung des Fragebogens enthält der Datensatz zusätzlich demographische Informationen über das Alter, Geschlecht, Wohnsituation sowie Bildungsgrad der 104 teilnehmenden Personen. In der folgenden Tabelle erhaltet ihr eine Übersicht über die Items des Konstrukts.\n\n\n\nÜbersichtstabelle der Items zu Umweltbewusstsein\n\n\n\n\n\nub_01 ~ Ich trenne Müll\n\n\nub_02 ~ Ich stecke nicht-benutzte Elektrogeräte aus\n\n\nub_03 ~ Ich nutze Strom aus erneuerbaren Energien\n\n\nub_04 ~ Ich nutze den ÖPNV\n\n\nub_05 ~ Ich schalte das Licht in nicht-genutzten Räumen aus\n\n\nub_06 ~ Ich achte beim Einkauf von Elektrogeräten auf Umweltsiegel\n\n\nub_07 ~ Wenn ich lüfte, mache ich die Heizung aus\n\n\nub_08 ~ Ich fahre sprit-sparend Auto\n\n\nub_09 ~ Ich achte beim Klamottenkauf auf kurze Lieferwege\n\n\nub_10 ~ Ich verkaufe/spende/ verschenke gebrauchte Gegenstände\n\n\nub_11 ~ Ich beschreibe Vorder- und Rückseite eines Blattes\n\n\nub_12 ~ Ich benutze Einmal-Hygiene Artikel\n\n\nub_13 ~ Bei Kurzstrecken laufe ich\n\n\nub_14 ~ Ich esse Fleisch\n\n\nub_15 ~ Ich kaufe regionale Lebensmittel\n\n\nub_16 ~ Ich kaufe unverpackte Lebensmittel\n\n\nub_17 ~ Ich werfe Lebensmittel weg\n\n\nub_18 ~ Ich kaufe Second-Hand-Kleidung\n\n\nub_19 ~ Ich achte beim Kauf von Hygieneartikeln auf biologisch abbaubare Inhaltsstoffe\n\n\nub_20 ~ Ich verzichte auf tierische Produkte\n\n\nub_21 ~ Ich nehme an Food-Sharing-Projekten teil\n\n\nub_22 ~ Ich lasse beim Spülen das Wasser laufen\n\n\nub_23 ~ An öffentlichen Orten hinterlasse ich Müll\n\n\nub_24 ~ Ich benutze wieder-verwendbare Behältnisse\n\n\nub_25 ~ Ich bewege mich bei Kurstrecken mit dem Fahrrad fort\n\n\nub_26 ~ Ich fliege Kurzstrecken\n\n\nub_27 ~ Ich esse Fleisch aus Massentierhaltung\n\n\nub_28 ~ Ich kaufe Second-Hand-Möbel\n\n\nub_29 ~ Ich fliege in den Urlaub"
  },
  {
    "objectID": "script_itemanalysis.html#einlesen-der-daten",
    "href": "script_itemanalysis.html#einlesen-der-daten",
    "title": "1  Itemanalyse",
    "section": "1.2 Einlesen der Daten",
    "text": "1.2 Einlesen der Daten\nDer Datensatz ist in diesem Fall als .rds-Datei gespeichert und kann mittels der Funktion (readRDS()) eingelesen werden.\n\n\n\n\n\n\nInformation zum Einlesen von Daten\n\n\n\nWenn ihr euren Datensatz aus SoSci-Survey herunterladet, wird dieser in einem anderen Format gespeichert sein (typischerweise entweder .csv, oder .xlsx). Um csv-Dateien (mit entweder -Komma, -Semikolon oder Tab-Separator) einzulesen, können entweder Base-R Funktionen (read.csv(), read.csv2() bzw. read.delim()) oder z.B. Funktionen des readr-Packages aus der Familie des tidyverse genutzt werden (read_csv(), read_csv2() und read_tsv()). Um Excel-Dateien einlesen zu können, kann das readxl-Package genutzt werden (read_excel())\n\n\nIhr habt die Möglichkeit, den Datensatz entweder lokal von euren Computer einzulesen oder alternativ einen Permalink aus dem dazugehörigen GitHub-Repository zu verwenden (dafür ist natürlich eine Internetverbindung erforderlich).\n\nMöglichkeit I: LokalMöglichkeit II: Link\n\n\nLadet den Datensatz aus OLAT herunter und stellt euer Working Directory richtig ein (setwd()). Danach kann der Datensatz folgendermaßen eingelesen werden readRDS(\"data_ub_raw.rds\"). Alternativ wäre eine allgemeine Empfehlung R-Projects zu nutzen, dann müsst ihr nicht mehr euer Working Directory einstellen und könnt relative Dateipfade nutzen readRDS(\"path/to/data/data_ub_raw.rds\"). Wer daran interessiert ist, findet unter diesem Link weitere Ressourcen.\n\nlibrary(here) # Bei Nutzung von R-Projects zur Nutzung von relativen Pfaden (statt setwd())\n\nhere() starts at /Users/luca/PowerFolders/Hiwi_Arbeit/DPPD/PsyBSc8_Diagnostik_WS2324\n\ndata_raw &lt;- readRDS(here::here(\"data/raw/data_ub_raw.rds\"))\n\n\n\nDurch Verwendung eines Permalinks können die Daten direkt von GitHub abgerufen werden. Dazu muss lediglich der Link als String in die Funktion url() eingegeben werden und kann anschließend wie gewohnt eingelesen werden.\n\npermalink &lt;- \"https://github.com/jlschnatz/PsyBSc8_Diagnostik_WS2324/raw/95ba618424d0465f7e0bfea4cc5ac840d82ad74e/data/raw/data_ub_raw.rds\"\nif (curl::has_internet()) {\n  data_raw &lt;- readRDS(url(permalink))\n}\n\n\n\n\nWir laden den gesamten Datensatz und speichern zudem ein Subset des Datensatzes, welches nur die Items zu Umweltbewusstsein enthält. Hilfreiche Funktionen, um einen ersten Überblick zu erhalten, sind zum Beispiel skim() oder str().\n\nlibrary(dplyr) # Datenmanipulation\nlibrary(skimr) # Hilfreich für Überblick von Dataframes\n\ndata_item &lt;- data_raw %&gt;% dplyr::select(ub_01:ub_29) # oder select(starts_with(\"ub_\"))\ndata_item &lt;- subset(data_raw, select = ub_01:ub_29) # Alternative\n\n# Überblick über den Datensatz\nskimr::skim(data_raw)\n\n\nData summary\n\n\nName\ndata_raw\n\n\nNumber of rows\n104\n\n\nNumber of columns\n35\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n30\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nid_case\n0\n1.00\nFALSE\n104\n63: 1, 64: 1, 67: 1, 70: 1\n\n\ndd_sex\n3\n0.97\nFALSE\n2\nWei: 67, Män: 34, Div: 0\n\n\ndd_wohn1\n3\n0.97\nFALSE\n4\nMit: 36, Mit: 28, In : 19, All: 18\n\n\ndd_wohn2\n3\n0.97\nFALSE\n4\nStä: 40, Län: 27, Ehe: 21, Ehe: 13\n\n\ndd_bildung\n3\n0.97\nFALSE\n5\nAbi: 44, Hoc: 26, Ber: 17, Rea: 11\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndd_alter\n3\n0.97\n31.27\n15.12\n14\n20\n24\n43\n81\n▇▂▂▂▁\n\n\nub_01\n3\n0.97\n4.09\n0.86\n0\n4\n4\n5\n5\n▁▁▁▇▃\n\n\nub_02\n3\n0.97\n2.50\n1.38\n0\n1\n2\n4\n5\n▇▅▃▆▂\n\n\nub_03\n3\n0.97\n2.29\n1.34\n0\n1\n2\n3\n5\n▆▇▆▂▂\n\n\nub_04\n3\n0.97\n2.56\n1.68\n0\n1\n3\n4\n5\n▇▃▅▅▃\n\n\nub_05\n3\n0.97\n4.33\n0.81\n0\n4\n4\n5\n5\n▁▁▂▇▇\n\n\nub_06\n3\n0.97\n2.69\n1.62\n0\n1\n3\n4\n5\n▇▃▆▅▅\n\n\nub_07\n3\n0.97\n3.26\n1.84\n0\n2\n4\n5\n5\n▆▂▂▆▇\n\n\nub_08\n3\n0.97\n3.00\n1.51\n0\n2\n3\n4\n5\n▆▆▆▇▆\n\n\nub_09\n3\n0.97\n1.45\n1.28\n0\n1\n1\n2\n5\n▇▃▂▁▁\n\n\nub_10\n3\n0.97\n2.99\n1.36\n0\n2\n3\n4\n5\n▃▇▇▇▅\n\n\nub_11\n3\n0.97\n3.38\n1.37\n0\n3\n4\n4\n5\n▃▅▇▇▇\n\n\nub_12\n3\n0.97\n2.22\n1.40\n0\n1\n2\n3\n5\n▇▇▆▂▂\n\n\nub_13\n3\n0.97\n3.66\n1.28\n0\n3\n4\n5\n5\n▂▂▆▆▇\n\n\nub_14\n3\n0.97\n2.05\n1.40\n0\n1\n2\n3\n5\n▇▆▇▂▁\n\n\nub_15\n3\n0.97\n2.86\n0.74\n1\n2\n3\n3\n5\n▁▃▇▂▁\n\n\nub_16\n3\n0.97\n2.29\n0.86\n0\n2\n2\n3\n4\n▁▃▇▇▁\n\n\nub_17\n3\n0.97\n3.44\n0.68\n2\n3\n3\n4\n5\n▂▇▁▇▁\n\n\nub_18\n3\n0.97\n1.18\n1.15\n0\n0\n1\n2\n4\n▇▇▃▂▁\n\n\nub_19\n3\n0.97\n2.19\n1.47\n0\n1\n2\n3\n5\n▇▅▅▂▂\n\n\nub_20\n3\n0.97\n2.00\n1.43\n0\n1\n2\n3\n5\n▇▆▃▁▂\n\n\nub_21\n3\n0.97\n0.51\n0.89\n0\n0\n0\n1\n3\n▇▂▁▂▁\n\n\nub_22\n3\n0.97\n1.21\n1.24\n0\n0\n1\n2\n5\n▇▃▁▁▁\n\n\nub_23\n3\n0.97\n3.12\n0.35\n3\n3\n3\n3\n5\n▇▁▁▁▁\n\n\nub_24\n3\n0.97\n3.78\n0.93\n0\n3\n4\n4\n5\n▁▁▅▇▃\n\n\nub_25\n3\n0.97\n2.20\n1.65\n0\n1\n2\n3\n5\n▇▆▅▂▃\n\n\nub_26\n3\n0.97\n0.33\n0.81\n0\n0\n0\n0\n5\n▇▁▁▁▁\n\n\nub_27\n3\n0.97\n1.27\n1.19\n0\n0\n1\n2\n5\n▇▃▁▁▁\n\n\nub_28\n3\n0.97\n1.17\n1.18\n0\n0\n1\n2\n4\n▇▆▅▂▁\n\n\nub_29\n3\n0.97\n1.58\n1.13\n0\n1\n2\n2\n5\n▇▆▂▁▁\n\n\n\n\n\nBevor wir mit der deskriptiven Analyse beginnen, sollten wir noch überprüfen, ob es fehlende Werte (NAs) im Datensatz gibt.\n\nanyNA(data_item)\n\n[1] TRUE\n\nsum(is.na(data_item)) # Alternative\n\n[1] 87\n\n\nIn diesem Fall sind 87 fehlende Werte vorhanden. Um die fehlenden Werte genauer zu explorieren, sollte am Rande das naniar-Package erwähnt werden, da es einige nützliche Funktionen diesbezüglich enthält. Wir können zum Beispiel über die cases (Proband:innen) hinweg die prozentuale Häufigkeit an Missings visualisieren.\n\nnaniar::gg_miss_case(data_item, show_pct = TRUE, order_cases = FALSE)\n\n\n\n\n\n\n\n\nWir sehen, dass drei Proband:innen den kompletten Fragebogen nicht ausgefüllt haben, weswegen mit der Funktion na.omit() ausgeschlossen werden.\n\n\n\n\n\n\nDisclaimer zu fehlenden Werten\n\n\n\nEs sollte zumindest am Rande erwähnt werden, dass die Funktion na.omit() sehr mächtig ist und nur mit Vorsicht verwendet werden sollte. Ohne weitere Überlegungen NAs auszuschließen, deren Fehlen möglicherweise nicht zufällig ist, sondern durch andere (nicht)-erhobene Variablen bedingt sind, kann zu Verzerrungen führen. Im Zweifel sprecht ihr euch am besten mit eurer/eurem Dozent:in ab, wie ihr mit fehlenden Werten umgehen sollt. Falls ihr euch mehr mit dem Thema NAs auseinandersetzen wollt, hier ein spannender Blogpost.\n\n\n\ndata_item &lt;- na.omit(data_item)\nanyNA(data_item)\n\n[1] FALSE"
  },
  {
    "objectID": "script_itemanalysis.html#deskriptive-analyse",
    "href": "script_itemanalysis.html#deskriptive-analyse",
    "title": "1  Itemanalyse",
    "section": "1.3 Deskriptive Analyse",
    "text": "1.3 Deskriptive Analyse\nBevor wir die Itemanalyse durchführen, wollen wir uns zunächst ein wenig mit den Daten vertraut machen. Hierfür benötigen wir zwei Packages: Das psych-Package beinhaltet sehr viele Funktionen und Befehle, die auch für viele andere Analysen hilfreich sind. Das Package janitor ist eine bessere Alternative zum Basis-Befehl table() und ist besonders informativ bei Häufigkeitstabellen.\n\n\n\n\n\n\nTipp für die Präsentationen & Hausarbeit\n\n\n\nFür die Abschlussberichte, braucht ihr die ganzen deskriptiven Informationen in APA7 formatierten Tabellen. Hierfür eignet sich besonders das Package sjPlot. Als Beispiel speichern wir zunächst die die vorherige deskriptive Statistik aller Items als ein Objekt ab. Danach erstellen wir mit einer Funktion des genannten Packages eine schön formatierte Tabelle. Die erstellte Tabelle kann sogar direkt als Word-Dokument abgespeichert werden, um danach noch weiter angepasst zu werden (z.B. Erstellen von Fußnoten, Tabellen-Titel, etc.). Wichtig dabei ist, dass nur die Endung .doc und nicht .docx funktioniert.\n\n\nFür die Berechnung deskriptiver Kennwerte (Mittelwert, Standardabweichung, Median, etc.) können wir die describe() Funktion des psych-Packages verwenden:\n\nlibrary(psych)\nlibrary(sjPlot)\ndescr_data_item &lt;- psych::describe(data_item)\nsjPlot::tab_df(\n  x = descr_data_item, \n  show.rownames = TRUE,\n  #file = table_descr_item.doc # Speichern als .doc Datei\n    )\n\n\n\n\nRow\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\nub_01\n1\n101\n4.09\n0.86\n4\n4.21\n0.00\n0\n5\n5\n-2.21\n8.27\n0.09\n\n\nub_02\n2\n101\n2.50\n1.38\n2\n2.46\n1.48\n0\n5\n5\n0.13\n-1.27\n0.14\n\n\nub_03\n3\n101\n2.29\n1.34\n2\n2.26\n1.48\n0\n5\n5\n0.23\n-0.46\n0.13\n\n\nub_04\n4\n101\n2.56\n1.68\n3\n2.58\n1.48\n0\n5\n5\n-0.05\n-1.29\n0.17\n\n\nub_05\n5\n101\n4.33\n0.81\n4\n4.46\n1.48\n0\n5\n5\n-1.86\n6.44\n0.08\n\n\nub_06\n6\n101\n2.69\n1.62\n3\n2.74\n1.48\n0\n5\n5\n-0.11\n-1.22\n0.16\n\n\nub_07\n7\n101\n3.26\n1.84\n4\n3.44\n1.48\n0\n5\n5\n-0.70\n-1.04\n0.18\n\n\nub_08\n8\n101\n3.00\n1.51\n3\n3.10\n1.48\n0\n5\n5\n-0.41\n-0.83\n0.15\n\n\nub_09\n9\n101\n1.45\n1.28\n1\n1.30\n1.48\n0\n5\n5\n0.93\n0.52\n0.13\n\n\nub_10\n10\n101\n2.99\n1.36\n3\n3.06\n1.48\n0\n5\n5\n-0.34\n-0.50\n0.14\n\n\nub_11\n11\n101\n3.38\n1.37\n4\n3.52\n1.48\n0\n5\n5\n-0.62\n-0.35\n0.14\n\n\nub_12\n12\n101\n2.22\n1.40\n2\n2.16\n1.48\n0\n5\n5\n0.27\n-0.48\n0.14\n\n\nub_13\n13\n101\n3.66\n1.28\n4\n3.81\n1.48\n0\n5\n5\n-0.85\n0.24\n0.13\n\n\nub_14\n14\n101\n2.05\n1.40\n2\n2.01\n1.48\n0\n5\n5\n-0.04\n-0.84\n0.14\n\n\nub_15\n15\n101\n2.86\n0.74\n3\n2.83\n0.00\n1\n5\n4\n0.22\n0.48\n0.07\n\n\nub_16\n16\n101\n2.29\n0.86\n2\n2.32\n1.48\n0\n4\n4\n-0.30\n-0.30\n0.09\n\n\nub_17\n17\n101\n3.44\n0.68\n3\n3.48\n1.48\n2\n5\n3\n-0.23\n-0.37\n0.07\n\n\nub_18\n18\n101\n1.18\n1.15\n1\n1.02\n1.48\n0\n4\n4\n0.90\n0.03\n0.11\n\n\nub_19\n19\n101\n2.19\n1.47\n2\n2.14\n1.48\n0\n5\n5\n0.23\n-0.89\n0.15\n\n\nub_20\n20\n101\n2.00\n1.43\n2\n1.90\n1.48\n0\n5\n5\n0.43\n-0.49\n0.14\n\n\nub_21\n21\n101\n0.51\n0.89\n0\n0.33\n0.00\n0\n3\n3\n1.51\n1.02\n0.09\n\n\nub_22\n22\n101\n1.21\n1.24\n1\n1.02\n1.48\n0\n5\n5\n0.93\n0.24\n0.12\n\n\nub_23\n23\n101\n3.12\n0.35\n3\n3.01\n0.00\n3\n5\n2\n2.96\n8.63\n0.04\n\n\nub_24\n24\n101\n3.78\n0.93\n4\n3.85\n1.48\n0\n5\n5\n-0.80\n1.34\n0.09\n\n\nub_25\n25\n101\n2.20\n1.65\n2\n2.12\n1.48\n0\n5\n5\n0.22\n-1.05\n0.16\n\n\nub_26\n26\n101\n0.33\n0.81\n0\n0.14\n0.00\n0\n5\n5\n3.43\n13.66\n0.08\n\n\nub_27\n27\n101\n1.27\n1.19\n1\n1.12\n1.48\n0\n5\n5\n0.96\n0.95\n0.12\n\n\nub_28\n28\n101\n1.17\n1.18\n1\n1.02\n1.48\n0\n4\n4\n0.74\n-0.38\n0.12\n\n\nub_29\n29\n101\n1.58\n1.13\n2\n1.51\n1.48\n0\n5\n5\n0.73\n0.79\n0.11\n\n\n\n\n\n\n\nWenn wir nur eine spezifische Variable deskriptiv betrachten wollen (z.B. dd_alter), kann in der gleichen Funktion die Variable direkt angesteuert werden.\n\ndescr_age &lt;- psych::describe(data_raw$dd_alter)\nsjPlot::tab_df(descr_age)\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n31.27\n15.12\n24\n29.28\n7.41\n14\n81\n67\n1.07\n-0.02\n1.50\n\n\n\n\n\n\n\nFür alle kategoriellen Daten (z.B. Geschlecht, Wohnort, Bildung) benötigen keine Mittelwerte oder Standardabweichungen, sondern nutzen Häufigkeitsverteilung zur deskriptiven Beschreibung. Hier kommt jetzt das janitor-Package zum Einsatz.\n\nlibrary(janitor)\ndescr_sex &lt;- tabyl(data_raw$dd_sex, show_na = FALSE)\ntab_df(descr_sex)\n\n\n\n\ndata_raw.dd_sex\nn\npercent\n\n\nMännlich\n34\n0.34\n\n\nWeiblich\n67\n0.66\n\n\nDivers\n0\n0.00\n\n\n\n\n\n\n\nWir bekommen die relativen und absoluten Häufigkeiten für männliche und weibliche Probanden ausgegeben. Falls es fehlenden Werte gäbe, müssten diese im Bericht auch angegeben werden. Dies ist ebenfalls mit der gleichen Funktion durch die Spezifizierung eines zusätzlichen Arguments möglich.\n\ndescr_sex_na &lt;- tabyl(data_raw$dd_sex, show_na = TRUE)\ntab_df(descr_sex_na)\n\n\n\n\ndata_raw.dd_sex\nn\npercent\nvalid_percent\n\n\nMännlich\n34\n0.33\n0.34\n\n\nWeiblich\n67\n0.64\n0.66\n\n\nDivers\n0\n0.00\n0.00\n\n\nNA\n3\n0.03\nNA\n\n\n\n\n\n\n\nZudem können wir mit dem psych-Package auch eine Tabelle nach Gruppen erstellen. Dieser Output kann dann mit einer ähnlichen Funktion des sjPlot-Package in einer Tabelle dargestellt werden.\n\ndescr_age_by_sex &lt;- describeBy(\n  x = data_raw$dd_alter, group = droplevels(data_raw$dd_sex) # Nicht angegebenes Level (Divers) des Faktors entfernen\n  )\n\ntab_dfs(\n  x = descr_age_by_sex,\n  titles = c(\"Männlich\",\"Weiblich\")\n  )\n\n\nMännlich\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n34\n26.65\n10.20\n23.50\n24.93\n4.45\n14\n58\n44\n1.72\n2.57\n1.75\n\n\n\n \n\nWeiblich\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n67\n33.61\n16.67\n25\n32\n8.90\n14\n81\n67\n0.75\n-0.76\n2.04\n\n\n\n\n\n\n\nEs gibt auch die Möglichkeit mehrere Tabellen in ein Dokument zu packen und diese in einem Word-Dokument zu speichern:\n\ntab_dfs(\n  x = list(descr_age, descr_sex_na),\n  titles = c(\"Descriptives of Age\", \"Descriptives of Sex\")\n  )\n\n\nDescriptives of Age\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n31.27\n15.12\n24\n29.28\n7.41\n14\n81\n67\n1.07\n-0.02\n1.50\n\n\n\n \n\nDescriptives of Sex\n\n\ndata_raw$dd_sex\nn\npercent\nvalid_percent\n\n\nMännlich\n34\n0.33\n0.34\n\n\nWeiblich\n67\n0.64\n0.66\n\n\nDivers\n0\n0.00\n0.00\n\n\nNA\n3\n0.03\nNA\n\n\n\n\n\n\n\n\ntab_dfs(\n  x = list(descr_age, descr_sex),\n  titles = c(\"Descriptives of Age\",\"Descriptives of Sex\"),\n  file = \"descriptives_all.doc\" # wieder als .doc abspeichern\n  )"
  },
  {
    "objectID": "script_itemanalysis.html#itemanalyse",
    "href": "script_itemanalysis.html#itemanalyse",
    "title": "1  Itemanalyse",
    "section": "1.4 Itemanalyse",
    "text": "1.4 Itemanalyse\n\n1.4.1 Rekodierung inverser Items\nFür die Itemanalyse benötigen wir den Datensatz in denen nur die Items vorhanden sind. Diesen haben wir bereits in einem vorherigen Schritt erstellt. Bevor wir die Itemanalyse jedoch durchführen, müssen wir alle Items, die inverse kodiert sind rekodieren.\n\n\n\n\n\n\nRekodierung leicht gemacht!\n\n\n\nDafür speichern wir alle inversen Items zunächst in einem Vektor ab. Anschließend verwenden wir die mutate() Funktion des dplyr-Package, mit welcher wir Variablen manupulieren/verändern können. Wir müssen dabei den Zusatz across() hinzunehmen, da wir mehreren Variablen gleichzeitig verändern wollen. Das Argument .cols gibt dabei an, welche Variablen wir verändern wollen. Mit dem Argument .fns spezifizieren wir, welche Funktion wir auf die Variablen anwenden wollen. Wir verwenden die Funktion rec() aus dem sjmisc-Package. Die etwas ungewöhnliche Schreibweise mit der Tilde ~und dem .x setzt sich wie folgt zusammen: Die Tilde müssen wir immer dann verwenden, wenn wir bei der Funktion, die wir anwenden zusätzlich Argumente spezifizieren (rec = \"rev\"). Das .x verwenden wir als Platzhalter für alle Variablen, die wir verändern wollen. Schließlich können wir mit dem .names Argument einen Namen für alle veränderten Variablen spezifizieren. Das Prefix {col} steht dabei für den ursprünglichen Variablenname. Mit dem Zusatz {col_r} wird hängen wir dem Präfix noch ein Suffix an. Das Suffix kennzeichnet dabei, dass wir die Items rekodiert haben.\n\n\n\nlibrary(sjmisc)\ninverse_items &lt;- paste0(\"ub_\", c(12, 14, 17, 22, 23, 26, 27, 29))\n\ndata_item_rec &lt;- data_item %&gt;%\n  mutate(across(\n    .cols = all_of(inverse_items), \n    .fns = ~sjmisc::rec(.x, rec = \"rev\"),\n    .names = \"{.col}_r\"\n    )) %&gt;%\n  select(-all_of(inverse_items))\n\ncolnames(data_item_rec)\n\n [1] \"ub_01\"   \"ub_02\"   \"ub_03\"   \"ub_04\"   \"ub_05\"   \"ub_06\"   \"ub_07\"  \n [8] \"ub_08\"   \"ub_09\"   \"ub_10\"   \"ub_11\"   \"ub_13\"   \"ub_15\"   \"ub_16\"  \n[15] \"ub_18\"   \"ub_19\"   \"ub_20\"   \"ub_21\"   \"ub_24\"   \"ub_25\"   \"ub_28\"  \n[22] \"ub_12_r\" \"ub_14_r\" \"ub_17_r\" \"ub_22_r\" \"ub_23_r\" \"ub_26_r\" \"ub_27_r\"\n[29] \"ub_29_r\"\n\n\nWir sehen, dass die Variablen jetzt nicht mehr in der “richtigen” Reihenfolge sind, da alle rekodierten Items am Ende des Dataframes auftauchen. Wir können die ursprüngliche Reihenfolge der Items wiederherstellen, indem wir diese in einem Vektor spezifizieren. Anschließend verwenden wir wieder die select() Funktion und bringen dadurch die Variablen in die gewünschte Reihenfolge.\n\ncol_order &lt;- sort(colnames(data_item_rec))\ndata_item_rec &lt;- select(data_item_rec, all_of(col_order))\ncolnames(data_item_rec)\n\n [1] \"ub_01\"   \"ub_02\"   \"ub_03\"   \"ub_04\"   \"ub_05\"   \"ub_06\"   \"ub_07\"  \n [8] \"ub_08\"   \"ub_09\"   \"ub_10\"   \"ub_11\"   \"ub_12_r\" \"ub_13\"   \"ub_14_r\"\n[15] \"ub_15\"   \"ub_16\"   \"ub_17_r\" \"ub_18\"   \"ub_19\"   \"ub_20\"   \"ub_21\"  \n[22] \"ub_22_r\" \"ub_23_r\" \"ub_24\"   \"ub_25\"   \"ub_26_r\" \"ub_27_r\" \"ub_28\"  \n[29] \"ub_29_r\"\n\n\n\n\n1.4.2 Itemanalyse I: Schwierigkeit\nJetzt können wir die Itemanalyse durchführen. Wir verwenden dafür eine Funktion aus dem sjPlot-Package:\n\n\n\n\n\n\nSchritt I\n\n\n\nIm ersten Schritt der Itemanalyse schauen wir uns die Schwierigkeiten der Items an und schließen Items aus, die zu schwer beziehungsweise zu leicht sind. Eine Faustregel, die auch in den Präsentationsfolien zur Itemanalyse erwähnt wurde, besagt, dass wir Items beibehalten sollten, bei denen 0.2 &lt; P_i &lt; 0.8 gilt. Dabei steht P_i für die Itemschwierigkeit. Von dieser Faustregel kann jedoch auch abgewichen werden bei der Erfassung von Konstrukten mit extremer Streuung oder wenn gezielt besonders schwere oder leichte Items im Konstrukt enthalten sein sollen (.05 &lt; P_i &lt; 0.95)\n\n\n\nitem_analysis_1 &lt;- tab_itemscale(\n  df = data_item_rec,\n  factor.groups.titles = \"Erste Itemanalyse\"\n  )\n\nitem_analysis_1\n\n\nErste Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nub_01\n0.00 %\n4.09\n0.86\n-2.28\n0.82\n0.33\n0.75\n\n\n\nub_02\n0.00 %\n2.5\n1.38\n0.14\n0.50\n0.25\n0.75\n\n\n\nub_03\n0.00 %\n2.29\n1.34\n0.24\n0.46\n0.24\n0.75\n\n\n\nub_04\n0.00 %\n2.56\n1.68\n-0.05\n0.51\n0.15\n0.76\n\n\n\nub_05\n0.00 %\n4.33\n0.81\n-1.92\n0.87\n0.17\n0.75\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.35\n0.74\n\n\n\nub_07\n0.00 %\n3.26\n1.84\n-0.72\n0.65\n0.28\n0.75\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.38\n0.74\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.32\n0.74\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.37\n0.74\n\n\n\nub_11\n0.00 %\n3.38\n1.37\n-0.64\n0.68\n0.28\n0.75\n\n\n\nub_12_r\n0.00 %\n2.78\n1.4\n-0.27\n0.56\n0.09\n0.76\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.43\n0.74\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.50\n0.73\n\n\n\nub_15\n0.00 %\n2.86\n0.74\n0.22\n0.57\n0.15\n0.75\n\n\n\nub_16\n0.00 %\n2.29\n0.86\n-0.31\n0.57\n0.26\n0.75\n\n\n\nub_17_r\n0.00 %\n3.56\n0.68\n0.24\n0.71\n0.21\n0.75\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.30\n0.75\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.50\n0.73\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.33\n0.74\n\n\n\nub_21\n0.00 %\n0.51\n0.89\n1.56\n0.17\n0.22\n0.75\n\n\n\nub_22_r\n0.00 %\n3.79\n1.24\n-0.96\n0.76\n0.29\n0.75\n\n\n\nub_23_r\n0.00 %\n4.88\n0.35\n-3.05\n0.98\n0.31\n0.75\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.39\n0.74\n\n\n\nub_25\n0.00 %\n2.2\n1.65\n0.22\n0.44\n0.12\n0.76\n\n\n\nub_26_r\n0.00 %\n4.67\n0.81\n-3.53\n0.93\n0.10\n0.76\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.46\n0.74\n\n\n\nub_28\n0.00 %\n1.17\n1.18\n0.76\n0.29\n0.18\n0.75\n\n\n\nub_29_r\n0.00 %\n3.42\n1.13\n-0.75\n0.68\n0.00\n0.76\n\n\n\nMean inter-item-correlation=0.100 · Cronbach's α=0.755\n\n\n\n\n\n\n\nIn dieser Situation entscheiden wir uns dazu, nur diejenigen Items zu behalten, für die 0.2 &lt; P_i &lt; 0.8 gilt. Es gibt zwei Methoden, dies umzusetzen: Eine Möglichkeit besteht darin, den Output manuell zu überprüfen, um diejenigen Items zu identifizieren, die dieses Kriterium nicht erfüllen. Diese Items werden dann in einem Vektor gespeichert und anschließend ausgeschlossen. Eine andere, etwas elegantere Lösung, die programmatischer vorgeht, besteht darin, direkt in R diejenigen Items herauszufiltern, die gemäß diesem Kriterium ausgeschlossen werden sollen. Dafür machen wir uns zu Nutzen, dass die Funktion sjPlot::tab_itemscale() uns nicht nur eine schöne HTML-Tabelle ausgibt, sondern diese im Hintergrund auch mit ausgibt. Durch die Verwendung von der Funktion str() können wir genauer betrachten, welche Informationen im Hintergrund ausgegeben werden, wenn wir das Objekt item_analysis_1 aufrufen.\n\nstr(object = item_analysis_1, vec.len = 1, nchar.max = 30)\n\nList of 10\n $ page.style     : chr \"&lt;style&gt;\\nhtml\"| __truncated__\n $ page.content   : chr \"&lt;table&gt;\\n&lt;cap\"| __truncated__\n $ page.complete  : chr \"&lt;html&gt;\\n&lt;head\"| __truncated__\n $ knitr          : chr \"&lt;table style=\"| __truncated__\n $ file           : NULL\n $ viewer         : logi TRUE\n $ df.list        :List of 1\n  ..$ :'data.frame':    29 obs. of  7 variables:\n  .. ..$ Missings           : chr [1:29] \"0.00 %\" ...\n  .. ..$ Mean               : chr [1:29] \"4.09\" ...\n  .. ..$ SD                 : chr [1:29] \"0.86\" ...\n  .. ..$ Skew               : chr [1:29] \"-2.28\" ...\n  .. ..$ Item Difficulty    : num [1:29] 0.82 0.5 ...\n  .. ..$ Item Discrimination: num [1:29] 0.328 0.248 ...\n  .. ..$ &alpha; if deleted : num [1:29] 0.746 0.749 ...\n $ index.scores   :'data.frame':    101 obs. of  1 variable:\n  ..$ Score1: num [1:101] 2.69 2.69 ...\n  .. ..- attr(*, \"label\")= Named chr \"Mean icc=0.10\"| __truncated__\n  .. .. ..- attr(*, \"names\")= chr \"Score1\"\n $ cronbach.values:List of 1\n  ..$ : num 0.755\n $ ideal.item.diff:List of 1\n  ..$ : Named num [1:29] 0.6 0.6 ...\n  .. ..- attr(*, \"names\")= chr [1:29] \"ub_01\" ...\n - attr(*, \"class\")= chr \"sjTable\"\n\n\nWie ersichtlich ist, ist der Output als Liste strukturiert und wir benötigen den Eintrag df.list. Danach müssen wir erneut in die Liste indizieren (item_analysis_1$df.list[[1]]). Da wir in diesem Skript mehrmals auf genau diese Itemtabelle zugreifen möchten, ist es ratsam, eine kleine Funktion zu schreiben, die diese Aufgabe übernimmt.\n\nextract_itemtable &lt;- function(.data) {\n  tab &lt;- sjPlot::tab_itemscale(.data)$df.list[[1]] \n  # clean_names ändert die Spaltennamen in lowercase und tauscht Leerzeichen mit _\n  out &lt;- janitor::clean_names(cbind(id_item = rownames(tab), tab))\n  return(out)\n}\n\n\nhead(extract_itemtable(data_item_rec))\n\n      id_item missings mean   sd  skew item_difficulty item_discrimination\nub_01   ub_01   0.00 % 4.09 0.86 -2.28            0.82               0.328\nub_02   ub_02   0.00 %  2.5 1.38  0.14            0.50               0.248\nub_03   ub_03   0.00 % 2.29 1.34  0.24            0.46               0.243\nub_04   ub_04   0.00 % 2.56 1.68 -0.05            0.51               0.153\nub_05   ub_05   0.00 % 4.33 0.81 -1.92            0.87               0.166\nub_06   ub_06   0.00 % 2.69 1.62 -0.11            0.54               0.349\n      alpha_if_deleted\nub_01            0.746\nub_02            0.749\nub_03            0.750\nub_04            0.758\nub_05            0.753\nub_06            0.743\n\n\nJetzt können wir die filter() Funktion des dplyr-Packages nutzen, um die Zeilen zu ermitteln, die in das Auschlusskriterium fallen.\n\nstep1_kick_diff &lt;- extract_itemtable(data_item_rec) %&gt;%\n  filter(item_difficulty &lt; .2 | item_difficulty &gt; .8) %&gt;% # `|` ist der ODER Operator in R \n  dplyr::pull(id_item) # macht das selbe wie `$id_item`(indiziert in den dataframe)\n\nprint(step1_kick_diff)\n\n[1] \"ub_01\"   \"ub_05\"   \"ub_21\"   \"ub_23_r\" \"ub_26_r\"\n\n\nAnschließend können wir ein neues Objekt erstellen, indem wir die 5 Items aus step1_kick_diff ausgeschlossen haben.\n\ndata_item_s1 &lt;- dplyr::select(data_item_rec, -all_of(step1_kick_diff))\n# data_item_s1 &lt;- data_item_rec[, !colnames(data_item_rec) %in% step1_kick_diff] # Alternative\n\n\n\n1.4.3 Itemanalyse II: Trennschärfe\nNachdem wir in der ersten Itemanalyse hinsichtlich der Itemschwierigkeit fünf Items ausgeschlossen haben, folgt die zweite Itemanalyse hinsichtlich der Itemtrennschärfe.\n\n\n\n\n\n\nSchritt II\n\n\n\nAls nächsten Schritt der Itemanalyse widmen wir uns nun der Trennschärfe der Item, also der Übereinstimmung der Differenzierungsfähigkeit eines Items mit dem Testscore der restlichen Items des Fragebogens. Dabei gilt die Faustregel, dass idealerweise 𝑟_{𝑖𝑡(𝑖)} \\geq .4 sein sollte. Nach einer sorgfältigen Überprüfung können jedoch auch Items beibehalten werden, bei denen 𝑟_{𝑖𝑡(𝑖)} \\geq .3 liegt.\n\n\nIn dieser Situation entscheiden wir uns das liberalere Kriterium von 𝑟_{𝑖𝑡(𝑖)} \\geq .3 zu wählen.\n\nitem_analysis_2 &lt;- tab_itemscale(\n  df = data_item_s1,\n  factor.groups.titles = \"Zweite Itemanalyse\"\n  )\n\nitem_analysis_2\n\n\nZweite Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nub_02\n0.00 %\n2.5\n1.38\n0.14\n0.50\n0.23\n0.73\n\n\n\nub_03\n0.00 %\n2.29\n1.34\n0.24\n0.46\n0.21\n0.73\n\n\n\nub_04\n0.00 %\n2.56\n1.68\n-0.05\n0.51\n0.19\n0.74\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.33\n0.72\n\n\n\nub_07\n0.00 %\n3.26\n1.84\n-0.72\n0.65\n0.26\n0.73\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.37\n0.72\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.32\n0.73\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.38\n0.72\n\n\n\nub_11\n0.00 %\n3.38\n1.37\n-0.64\n0.68\n0.26\n0.73\n\n\n\nub_12_r\n0.00 %\n2.78\n1.4\n-0.27\n0.56\n0.05\n0.75\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.47\n0.71\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.51\n0.71\n\n\n\nub_15\n0.00 %\n2.86\n0.74\n0.22\n0.57\n0.14\n0.74\n\n\n\nub_16\n0.00 %\n2.29\n0.86\n-0.31\n0.57\n0.25\n0.73\n\n\n\nub_17_r\n0.00 %\n3.56\n0.68\n0.24\n0.71\n0.18\n0.73\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.33\n0.73\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.49\n0.71\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.34\n0.72\n\n\n\nub_22_r\n0.00 %\n3.79\n1.24\n-0.96\n0.76\n0.29\n0.73\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.40\n0.72\n\n\n\nub_25\n0.00 %\n2.2\n1.65\n0.22\n0.44\n0.14\n0.74\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.46\n0.72\n\n\n\nub_28\n0.00 %\n1.17\n1.18\n0.76\n0.29\n0.20\n0.73\n\n\n\nub_29_r\n0.00 %\n3.42\n1.13\n-0.75\n0.68\n-0.04\n0.75\n\n\n\nMean inter-item-correlation=0.107 · Cronbach's α=0.737\n\n\n\n\n\n\n\nWir greifen erneut auf die zuvor definierte Funktion extract_itemtable() zurück, um die erforderlichen Informationen zu extrahieren. Diesmal filtern wir nach der Variable item_discrimination. Den resultierenden Vektor speichern wir erneut als Objekt und nutzen ihn zur Erstellung eines endgültigen Datensatzes, der nur noch Variablen enthält, die sowohl angemessene Schwierigkeit als auch Trennschärfe aufweisen.\n\nstep2_kick_disc &lt;- extract_itemtable(data_item_s1) %&gt;%\n  dplyr::filter(item_discrimination &lt; .3) %&gt;%\n  dplyr::pull(id_item)\n\nprint(step2_kick_disc)\n\n [1] \"ub_02\"   \"ub_03\"   \"ub_04\"   \"ub_07\"   \"ub_11\"   \"ub_12_r\" \"ub_15\"  \n [8] \"ub_16\"   \"ub_17_r\" \"ub_22_r\" \"ub_25\"   \"ub_28\"   \"ub_29_r\"\n\ndata_item_s2 &lt;- dplyr::select(data_item_s1, -all_of(step2_kick_disc))\n\nMit diesem Datensatz können wir nun die finale Itemanalyse durchführen.\n\ntab_itemscale(\n  df = data_item_s2,\n  factor.groups.titles = \"Finale Itemanalyse\"\n  )\n\n\nFinale Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.41\n0.74\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.36\n0.75\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.35\n0.75\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.38\n0.74\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.35\n0.75\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.55\n0.72\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.23\n0.76\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.51\n0.72\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.43\n0.74\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.35\n0.75\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.56\n0.72\n\n\n\nMean inter-item-correlation=0.222 · Cronbach's α=0.758\n\n\n\n\n\n\n\n\n\n\n\n\n\nZusammenfassung\n\n\n\nZusammenfassend wurden im Zuge der Itemanalyse insgesamt 18 der 29 Items ausgeschlossen. Am Ende dieses Analyseschritts bleiben somit 11 Items übrig, die für weitere Analysen, insbesondere die explorative Faktorenanalyse (EFA), in Betracht gezogen werden (Details siehe EFA). Die durchschnittliche inter-item-Korrelation von .22 weist darauf hin, dass die Items im Fragebogen in einem moderaten Maß miteinander korrelieren. Dies deutet darauf hin, dass es eine gewisse gemeinsame Varianz zwischen den Items gibt, ohne dass sie übermäßig stark miteinander verbunden sind. Ein Cronbach’s \\alpha Wert von \\alpha = .75 lässt darauf schließen, dass der Fragebogen eine akzeptable Reliabilität aufweist."
  },
  {
    "objectID": "script_itemanalysis.html#zusatz",
    "href": "script_itemanalysis.html#zusatz",
    "title": "1  Itemanalyse",
    "section": "1.5 Zusatz",
    "text": "1.5 Zusatz\n\nAlternative Reliabilitätsberechnung: McDonald’s OmegaBerechnung der Itemvarianz\n\n\nAbschließend gibt es noch die Möglichkeit, McDonald´s \\omega als ein alternatives Reliabilitätsmaß (zusätzlich zu Cronbach´s \\alpha) zu bestimmen. Wir nutzen dafür wieder eine Funktion aus dem psych-Package.\n\nomega_items &lt;- psych::omega(data_item_s2, plot = FALSE)\nomega_items$omega.tot\n\n[1] 0.8306594\n\nomega_items$alpha\n\n[1] 0.7582772\n\n\n\n\nNeben der Berechnung der Itemschwierigkeit und Trennschärfe ist es sinnvoll, auch die Itemvarianz zu analysieren. Die Itemvarianz stellt zwar kein Selektionskriterium dar, da sie nicht standardisiert ist, jedoch ist sie wichtig, um festzustellen, ob Personen in ihrem Antwortverhalten bei einem bestimmten Item überhaupt variieren (ob die Itemvarianz &gt; 0 ist). Zudem ermöglicht die Itemvarianz Vergleiche zwischen verschiedenen Items.\n\ndiag(var(data_item_s2))\n\n    ub_06     ub_08     ub_09     ub_10     ub_13   ub_14_r     ub_18     ub_19 \n2.6148515 2.2800000 1.6295050 1.8499010 1.6455446 1.9675248 1.3279208 2.1742574 \n    ub_20     ub_24   ub_27_r \n2.0400000 0.8720792 1.4178218"
  },
  {
    "objectID": "script_itemanalysis.html#ausblick",
    "href": "script_itemanalysis.html#ausblick",
    "title": "1  Itemanalyse",
    "section": "1.6 Ausblick",
    "text": "1.6 Ausblick\nIm kommenden Skript setzen wir uns mit der Exploratorischen Faktorenanalyse (EFA) auseinander. Daher ist es sinnvoll, den Zwischenstand bzw. die Ergebnisse der Itemanalyse zu speichern, um nahtlos von diesem Punkt aus fortzufahren. Zu diesem Zweck sichern wir den finale Dataframe data_item_s2 als csv-Datei.\n\nreadr::write_csv(x = data_item_s2, file = here(\"data/processed\", \"data_item_itemanalysis.csv\"))\n# oder write.csv()"
  },
  {
    "objectID": "script_itemanalysis.html#session-info",
    "href": "script_itemanalysis.html#session-info",
    "title": "1  Itemanalyse",
    "section": "1.7 Session Info",
    "text": "1.7 Session Info\n\n\n\n\n\n\nErweitern für Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.1.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2023-11-27\n pandoc   3.1.4 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package * version date (UTC) lib source\n dplyr   * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n here    * 1.0.1   2020-12-13 [1] CRAN (R 4.3.0)\n janitor * 2.2.0   2023-02-02 [1] CRAN (R 4.3.0)\n psych   * 2.3.9   2023-09-26 [1] CRAN (R 4.3.1)\n sjmisc  * 2.8.9   2021-12-03 [1] CRAN (R 4.3.0)\n sjPlot  * 2.8.15  2023-08-17 [1] CRAN (R 4.3.0)\n skimr   * 2.1.5   2022-12-23 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "script_efa.html#laden-der-daten-übersicht",
    "href": "script_efa.html#laden-der-daten-übersicht",
    "title": "2  EFA",
    "section": "2.1 Laden der Daten & Übersicht",
    "text": "2.1 Laden der Daten & Übersicht\nWir beginnen mit dem Datensatz, mit dem wir letztes Skript aufgehört haben. Wie immer müsst ihr dafür den Pfad wählen, in dem sich die Daten befinden. Ihr könnt die finalen Daten aus der Itemeanalyse entweder wieder aus OLAT herunterladen (unter Gemeinsame Dokumente aller Gruppen/R-Skripte_Auswertung) oder direkt den GitHub-Permalink nutzen.\n\nMöglichkeit I – LokalMöglichkeit II - Link\n\n\n\nlibrary(here)\ndata_item_final &lt;- read.csv(here(\"data/processed/data_item_itemanalysis.csv\"))\n\n\n\n\nlink &lt;- url(\"https://raw.githubusercontent.com/jlschnatz/PsyBSc8_Diagnostik_WS2324/main/data/processed/data_item_itemanalysis.csv\")\ndata_item_final &lt;- read.csv(link)\n\n\n\n\nMit skim() können wir uns nochmal einen kurzen Überblick verschaffen, wie die Datenstruktur aussieht und welche Variablen wir nach der Itemanalyse beibehalten/ausgeschlossen haben.\n\nlibrary(skimr)\nskim(data_item_final)\n\n\nData summary\n\n\nName\ndata_item_final\n\n\nNumber of rows\n101\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nub_06\n0\n1\n2.69\n1.62\n0\n1\n3\n4\n5\n▇▃▆▅▅\n\n\nub_08\n0\n1\n3.00\n1.51\n0\n2\n3\n4\n5\n▆▆▆▇▆\n\n\nub_09\n0\n1\n1.45\n1.28\n0\n1\n1\n2\n5\n▇▃▂▁▁\n\n\nub_10\n0\n1\n2.99\n1.36\n0\n2\n3\n4\n5\n▃▇▇▇▅\n\n\nub_13\n0\n1\n3.66\n1.28\n0\n3\n4\n5\n5\n▂▂▆▆▇\n\n\nub_14_r\n0\n1\n2.95\n1.40\n0\n2\n3\n4\n5\n▃▇▇▂▆\n\n\nub_18\n0\n1\n1.18\n1.15\n0\n0\n1\n2\n4\n▇▇▃▂▁\n\n\nub_19\n0\n1\n2.19\n1.47\n0\n1\n2\n3\n5\n▇▅▅▂▂\n\n\nub_20\n0\n1\n2.00\n1.43\n0\n1\n2\n3\n5\n▇▆▃▁▂\n\n\nub_24\n0\n1\n3.78\n0.93\n0\n3\n4\n4\n5\n▁▁▅▇▃\n\n\nub_27_r\n0\n1\n3.73\n1.19\n0\n3\n4\n5\n5\n▁▂▇▇▇"
  },
  {
    "objectID": "script_efa.html#recap-der-itemanalyse",
    "href": "script_efa.html#recap-der-itemanalyse",
    "title": "2  EFA",
    "section": "2.2 Recap der Itemanalyse",
    "text": "2.2 Recap der Itemanalyse\nAls Erinnerung können wir uns nochmal die psychometrische Eigenschaften der beibehaltenen Items anschauen. Wir benutzen dafür wieder die Funktion tab_itemscale() aus dem sjPlot-Package, die wir letztes Skript kennengelernt haben.\n\nlibrary(sjPlot)\ntab_itemscale(df = data_item_final, factor.groups.titles = \"Finale Itemanalyse\")\n\n\nFinale Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.41\n0.74\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.36\n0.75\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.35\n0.75\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.38\n0.74\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.35\n0.75\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.55\n0.72\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.23\n0.76\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.51\n0.72\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.43\n0.74\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.35\n0.75\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.56\n0.72\n\n\n\nMean inter-item-correlation=0.222 · Cronbach's α=0.758"
  },
  {
    "objectID": "script_efa.html#durchführung-der-efa",
    "href": "script_efa.html#durchführung-der-efa",
    "title": "2  EFA",
    "section": "2.3 Durchführung der EFA",
    "text": "2.3 Durchführung der EFA\n\n2.3.1 Identifikation der Anzahl der Faktoren\nZur Bestimmung der Anzahl der Faktoren in der exploratorischen Faktorenanalyse gibt es mehrere Herangehensweisen, die bereits in dem Lernbar-Video vorgestellt wurden: das Eigenwertkriterium/Kaiser-Guttman-Kriterium [@kaiser1960application], der Scree-Plot [@cattell1966scree] und die Parallelanalyse [@horn1965rationale]. Jedes dieser Verfahren hat seine eigenen Vorbehalte hinsichtlich Konservativität und Objektivität. Deswegen ist es in der Praxis empfehlenswert, die Ergebnisse mehrerer Kriterien gleichzeitig zu berücksichtigen, um die höchste Genauigkeit zu erzielen. Für eine Übersicht hinsichtlich verschiedener Extraktionsmethoden und deren Vergleich siehe: @auerswald2019determine.\n\n2.3.1.1 Möglichkeit I - Eigenwertkriterium\nDas Eigenwertkriterium bzw. Kaiser-Kriterium ist das liberalste Maß der Entscheidung, weswegen tendeziell dadurch viele Faktoren entstehen. Zur Bestimmung werden die Eigenwerte errechnet und alle Faktoren beibehalten, deren Eigenwert \\lambda &gt;= 1 ist. Es gibt zwei Möglichkeiten, den Eigenwerteverlauf mittels R zu bestimmen.\n\n1. Weg: Base-R Funktion eigen()2. Weg: FactorMineR Package\n\n\nZunächst berechnen wir die symmetrische Korrelationsmatrix aus den Daten und können dann mit der eigen() Funktion die den Eigenwertverlauf bestimmen. Der Output ist eine Liste, in die wir indizieren müssen, um die Eigenwerte zu extrahieren.\n\n# Base R\neigen(cor(data_item_final))$values \n\n [1] 3.3374011 1.5944637 1.1764625 0.9872941 0.9171743 0.7127462 0.6879672\n [8] 0.6022027 0.4861623 0.3122628 0.1858631\n\nlibrary(tidyverse)\n# Alternative Schreibweise (mit pipes):\ncor(data_item_final) %&gt;% \n  eigen() %&gt;% \n  chuck(\"values\") # chuck() extrahiert einzelne Elemente aus Listen\n\n [1] 3.3374011 1.5944637 1.1764625 0.9872941 0.9171743 0.7127462 0.6879672\n [8] 0.6022027 0.4861623 0.3122628 0.1858631\n\n# in Tabellenform:\ncor(data_item_final) %&gt;% \n  eigen() %&gt;% \n  chuck(\"values\") %&gt;% \n  as_tibble() %&gt;% \n  rename(Eigenvalue = value) %&gt;% \n  tab_df(\n    title = \"Eigenwertverlauf\",\n    col.header = \"Eigenwert\",\n    show.rownames = TRUE\n    )\n\n\nEigenwertverlauf\n\n\nRow\nEigenwert\n\n\n1\n3.34\n\n\n2\n1.59\n\n\n3\n1.18\n\n\n4\n0.99\n\n\n5\n0.92\n\n\n6\n0.71\n\n\n7\n0.69\n\n\n8\n0.60\n\n\n9\n0.49\n\n\n10\n0.31\n\n\n11\n0.19\n\n\n\n\n\n\n\n\n\nMit dem alternativen Weg könnnen wir noch etwas mehr Informationen als nur die Eigenwerte extrahieren. Dazu müssen wir das FactoMineR Package laden bzw. wenn noch nicht geschehen auch installieren. Wir verwenden die PCA() Funktion und extrahieren im Anschluss die Eigenwerte und Varianzaufklärung der möglichen Faktoren aus der abgespeichertern Liste.\n\n# install.packages(\"FactoMineR\")\nlibrary(FactoMineR)\n\npca &lt;- PCA(data_item_final, graph = FALSE) \n\neigen &lt;- as.data.frame(pca$eig) # als dataframe abspeichern\neigen &lt;- as.data.frame(pca[[\"eig\"]]) # Alternative\n\n# in Tabellenform:\ntab_df(\n  x = eigen,\n  show.rownames = TRUE,\n  title = \"Eigenwertverlauf mit zusätzlicher Information hinsichtlich erklärte Varianz\",\n  col.header  = c(\"Eigenwert\", \"Erklärte Varianz\", \"Kum. erklärte Varianz\")\n  ) \n\n\nEigenwertverlauf mit zusätzlicher Information hinsichtlich erklärte Varianz\n\n\nRow\nEigenwert\nErklärte Varianz\nKum. erklärte Varianz\n\n\ncomp 1\n3.34\n30.34\n30.34\n\n\ncomp 2\n1.59\n14.50\n44.84\n\n\ncomp 3\n1.18\n10.70\n55.53\n\n\ncomp 4\n0.99\n8.98\n64.51\n\n\ncomp 5\n0.92\n8.34\n72.84\n\n\ncomp 6\n0.71\n6.48\n79.32\n\n\ncomp 7\n0.69\n6.25\n85.58\n\n\ncomp 8\n0.60\n5.47\n91.05\n\n\ncomp 9\n0.49\n4.42\n95.47\n\n\ncomp 10\n0.31\n2.84\n98.31\n\n\ncomp 11\n0.19\n1.69\n100.00\n\n\n\n\n\n\n\n\n\n\nWie anhand der Ergebnisse erkennbar ist, wird durch das Eigenwertkriterium ein Modell mit 3 Faktoren vorgeschlagen.\n\n\n2.3.1.2 Möglichkeit II - Scree-Plot\nDer Scree-Plot ist ein visuell deskriptives Kriterium zur Entscheidung der Faktoren. Dabei ist das Kriterium konservativer als das Eigenwertkriterium. Wie in dem Lernbar-Video beschrieben wird der optische Knick herangezogen, um die Entscheidung über die Faktorenanzahl zu treffen. Alle Faktoren, die sich “über” dem Knick befinden, werden beibehalten.\n\n\n\n\n\n\nWelches Package fürs Plotting?\n\n\n\nUm in R einen Scree-Plot zu generieren, könnnen wir entweder die Base-R Plotting Funktionen verwenden oder das sehr erfolgreiche ggplot2-Package der tidyverse Familie benutzen. Dabei können zwar mit Base R Plotting Funktionen zwar in nur wenigen Zeilen ein Plot erstellt werden, bieten dafür aber nicht so viele Anpassungsmöglichkeiten und folgen keiner klaren Struktur im Vergleich zu ggplot2. Wem es also wichtig sein sollte, schöne Plots zu generieren, sollte eher das ggplot2-Package nutzen. Wer nochmal eine ggplot2 Auffrischung brauchen sollte, findet hier ein paar nützliche Links: PandaR, R for Data Science & ggplot2-Buch.\n\n\n\n1. Weg: Base-R2. Weg: ggplot2 Package\n\n\nWir benutzen das psych Package, um den Scree-Plot zu erstellen. Dabei verwendet dieses Package im Hintergrund Base R für das Plotting.\n\nlibrary(psych)\nscree(\n  rx = data_item_final, \n  factors = TRUE,\n  pc = FALSE, # sollen Hauptkomponenten auch dargestellt werden?\n  hline = 1 # Plot mit Knicklinie, hline = -1 -&gt; ohne Linie\n  ) \n\n\n\n\n\n\n\n\n\n\nWir erstellen zunächst einen Dataframe, indem die Eigenwerte und Nummerierung der Faktoren als zwei Variablen gespeichert sind. Danach werden diese Daten in Layern mittels ggplot2 geplottet.\n\neigen_res &lt;- eigen(cor(data_item_final))\ndata_eigen &lt;- data.frame(\n  Eigenwert = eigen_res$values,\n  Faktor = seq_along(eigen_res$values) # alternativ 1:length(eigen_res$values)\n)\n\n# Basis-ggplot Layer\nggplot(\n  data = data_eigen, \n  mapping = aes(x = Faktor, y = Eigenwert) # Zuordnung von Variablen zu Koordinatenachsen\n  ) + \n  # Horizontale Linie bei y = 1\n   geom_hline(\n    color = \"darkgrey\", \n    yintercept = 1,\n    linetype = \"longdash\" \n    ) +\n  # Hinzufügen der Linien\n  geom_line(alpha = 0.6, color = \"royalblue4\") + \n  # Hinzufügen der Punkte\n  geom_point(size = 3, color = \"royalblue4\") +\n  # Achsenveränderung der y-Achse\n  scale_y_continuous(\n    breaks = seq(0, 4, 1),\n    limits = c(0, 4),\n    expand = c(0, 0)\n  ) +\n  # Achsenveränderung der x-Achse\n  scale_x_continuous(\n    breaks = seq(1, 11),\n    limits = c(1, 11)\n  ) +\n  # Theme\n  theme_sjplot() \n\n\n\n\n\n\n\n\n\n\n\nDie Ergebnisse des Scree-Plots sind in diesem Fall nicht ganz eindeutig interpretierbar, da kein klarer Knick im Verlauf erkennbar ist. Hier wird auch ein Nachteil dieser Methode deutlich: die visuelle Interpretation bleibt immer bis zu einem gewissen Grad subjektiv. Am ehesten scheint ein Modell mit zwei Faktoren gemäß dem Scree-Plot am sinnvollsten. Daher wäre die Entscheidung gemäß dem Scree-Plot im Vergleich zum Kaiser-Kriterium konservativer.\n\n\n2.3.1.3 Möglichkeit III - Parallelanalyse\nDie letzte vorgestellte Möglichkeit ist die Parallelanalyse. Dabei werden die Eigenwerte der Faktorenanalyse des realen Datensatzes mit denen eines Datensatzes mit normalverteilten unkorrelierten Zufallsdaten verglichen, wobei Faktoren beibehalten werden, deren Eigenwerte größer sind als die gemittelten Eigenwerte der Zufallsdaten [@Klopp_2010]. Anders formuliert werden Faktoren beibehalten, die mehr Varianz aufklären als rein zufällig simulierte Daten. Von den drei vorgestellten Kriterien ist diese das konservativste Kriterium.\n\nMöglichkeit I - psych-Package und base RMöglichkeit II - ggplot2\n\n\n\npsych::fa.parallel(\n  x = data_item_final, \n  fm = \"pa\", # Principal Axis Factoring Extraktion\n  fa = \"fa\", # Factor Analysis (fa = \"pc\" für Hauptkomponentenanalyse)\n  n.iter = 1000, # Anzahl der Simulationen\n  quant = .95, # Vergleichsintervall\n  main = \"Parallelanalyse mittels Base R und psych-Package\",\n  ylabel = \"Eigenwert\",\n  error.bars = FALSE # TRUE für Error-Bars\n  )\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n\n\n\nWir speichern zunächst die ausgegeben Daten der Funktion fa.parallel() in einem Objekt ab. Danach erstellen wir einen Dateframe mit den relevanten Informationen (empirische und simulierte Eigenwerte).\n\ndata_pa &lt;- fa.parallel(\n  x = data_item_final, \n  fm= \"pa\", \n  fa = \"fa\", \n  plot = FALSE,\n  n.iter = 1000,\n  quant = .95\n  )\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n# Dataframe\ndata_plot &lt;- data.frame(\n  Observed = data_pa$fa.values, # empirisch \n  Simulated = data_pa$fa.sim    # simuliert\n  )\n\n# Zeilennamen als eigene Variable \"Factor\"\ndata_plot$Factor &lt;- as.integer(rownames(data_plot))\n\ntab_df(data_plot)\n\n\n\n\nObserved\nSimulated\nFactor\n\n\n2.67\n0.80\n1\n\n\n0.88\n0.44\n2\n\n\n0.30\n0.31\n3\n\n\n0.22\n0.21\n4\n\n\n0.12\n0.11\n5\n\n\n-0.07\n0.03\n6\n\n\n-0.12\n-0.05\n7\n\n\n-0.23\n-0.13\n8\n\n\n-0.33\n-0.21\n9\n\n\n-0.36\n-0.30\n10\n\n\n-0.41\n-0.40\n11\n\n\n\n\n\n\n\n\nUm Daten mit ggplot2 zu visualisieren, müssen sie im Long-Format vorliegen. In diesem Kontext bedeutet das, dass jede Beobachtung durch drei Variablen repräsentiert wird: eine für den Faktor, eine zur Kennzeichnung, ob der Datenpunkt simuliert oder empirisch ist, und eine für die Eigenwerte.\nDie praktische Funktion pivot_longer() aus dem tidyr-Package ermöglicht die Transformation von Dataframes vom Wide-Format ins Long-Format (analog dazu kann pivot_wider() von Wide zu Long genutzt werden). Dabei können wir mit dem cols-Argument die zu transformierenden Variablen spezifizieren, und mit names_to und values_to geben wir den neu erstellten Variablen eigene Namen.\n\ndata_plot_long &lt;- pivot_longer(\n  data = data_plot,\n  cols = c(Observed, Simulated),\n  names_to = \"Typ\",\n  values_to = \"Eigenwert\"\n  )\n\ntab_df(data_plot_long)\n\n\n\n\nFactor\nTyp\nEigenwert\n\n\n1\nObserved\n2.67\n\n\n1\nSimulated\n0.80\n\n\n2\nObserved\n0.88\n\n\n2\nSimulated\n0.44\n\n\n3\nObserved\n0.30\n\n\n3\nSimulated\n0.31\n\n\n4\nObserved\n0.22\n\n\n4\nSimulated\n0.21\n\n\n5\nObserved\n0.12\n\n\n5\nSimulated\n0.11\n\n\n6\nObserved\n-0.07\n\n\n6\nSimulated\n0.03\n\n\n7\nObserved\n-0.12\n\n\n7\nSimulated\n-0.05\n\n\n8\nObserved\n-0.23\n\n\n8\nSimulated\n-0.13\n\n\n9\nObserved\n-0.33\n\n\n9\nSimulated\n-0.21\n\n\n10\nObserved\n-0.36\n\n\n10\nSimulated\n-0.30\n\n\n11\nObserved\n-0.41\n\n\n11\nSimulated\n-0.40\n\n\n\n\n\n\n\nEine alternative Methode, die ausschließlich auf base-R basiert, wird hier beschrieben.\n\ndata_plot_long &lt;- reshape(\n    data = data_plot,\n    varying = c(\"Observed\", \"Simulated\"), \n    v.names = \"Eigenwert\",\n    timevar = \"Typ\",\n    times = c(\"Observed\", \"Simulated\"),\n    direction = \"long\"\n  )\n\nJetzt können wir die Abbildung mit ggplot2 generieren. Dabei definieren wir neben x und y mit aes(color = Typ), dass diese Variable als Farbkodierung dargestellt werden soll.\n\nggplot(\n    # Daten\n    data = data_plot_long,\n    # Aesthetics\n    aes(x = Factor, y = Eigenwert, color = Typ)\n    ) + \n  # Linien\n  geom_line(\n    size = 0.7,\n    alpha = .8\n    ) + \n  # Punkte\n  geom_point(\n    size = 3.5,\n    alpha = .8\n    ) + \n  # Achsenveränderungen\n  scale_y_continuous(\n    breaks = seq(-1, 3, 1),\n    limits = c(-1, 3),\n    expand = c(0, 0)\n  ) +\n  scale_x_continuous(\n    breaks = seq(1, 11),\n    limits = c(1, 11)\n  ) + \n  # Farben verändern\n  scale_color_manual(values = c(\"royalblue4\", \"orangered2\")) +\n  # Themes\n  theme_sjplot() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nDie Ergebnisse der Parallelanalyse suggerieren ein Modell mit zwei Faktoren. Da dies das konservativste Vorgehen ist, entscheiden wir uns für ein zweifaktorielle Modell für die weitere Analyse.\n\n\n\n2.3.2 Extraktion der Faktoren & Rotation\nAls nächsten Schritt können wir nun die eigentliche exploratorische Faktoranalyse durchführen. Wir benutzen dafür wieder ein Befehl aus dem sjPlot-Package, mit dem wir direkt die Faktorstruktur als Tabelle darstellen können. Es gibt aber auch viele alternative Packages, mit denen eine EFA durchgeführt werden kann (z.B. psych fa() , mit dem parameters-Paket factor_analysis() oder base-R factanal()).\nWir spezifizieren ein Modell mit 2 Faktoren, die mittels Principal Axis Factoring extrahiert werden (method = \"pa\") und führen anschließend eine oblimine Rotation durch (rotation = \"oblimin\"). Wie in der Lernbar bereits vorgestellt, gibt es unterschiedliche Rotationsverfahren nach der Extraktion der Faktoren. Bei orthogonalen Rotation bleiben die latenten Faktoren unkorreliert, wohingegen bei obliquer Rotation die Faktoren miteinander korrelieren dürfen. Alle Rotationsverfahren haben das Ziel, möglichst eine Einfachstruktur zu erhalten, d.h. dass jedes Item möglichst nur auf einen Faktor lädt. Für einen Vergleich verschiedener Rotationsverfahren und wann welches angewendet werden sollte: @costello2005best.\n\nfit_fa &lt;- tab_fa(\n  data = data_item_final,\n  nmbr.fctr = 2, # Faktorenanzahl\n  rotation = \"oblimin\", # Rotationsverfahren\n  fctr.load.tlrn = 0, \n  method = \"pa\", # Alternative Methoden aus Lernbar, \"ml\" für Maximum-Likelihood \n  title = \"Faktorenanalyse\",\n  #file = \"fit_fa.doc\" # Ergebnisse können wieder als .doc gespeichert werden\n  )\n\nfit_fa\n\n\nFaktorenanalyse\n\n\n \nFactor 1\nFactor 2\n\n\nub_06\n-0.07\n0.69\n\n\nub_08\n0.01\n0.47\n\n\nub_09\n-0.09\n0.59\n\n\nub_10\n0.13\n0.37\n\n\nub_13\n0.26\n0.21\n\n\nub_14_r\n0.99\n-0.04\n\n\nub_18\n0.14\n0.16\n\n\nub_19\n0.19\n0.54\n\n\nub_20\n0.73\n-0.03\n\n\nub_24\n0.20\n0.29\n\n\nub_27_r\n0.62\n0.24\n\n\nCronbach's α\n0.76\n0.67\n\n\n\n\n\nDer Output besteht erneut aus einer HTML-Tabelle, die die Ladungen der 11 Items auf beide Faktoren sowie die Reliabilität der beiden Skalen anzeigt. Die jeweils höhere Ladung ist dabei hervorgehoben. Wir erinnern uns daran, dass bei der EFA es zwei Selektionskriterien zur Ermittlung der bestmöglichen Einfachstruktur gibt: hohe Faktorladungen (Auschluss von Item \\lambda &lt; .3 bzw. konservativer \\lambda &lt; .4) und Ausschluss von Items mit Doppelladungen (Cutoff z.B. bei \\mid~ \\lambda_1 - \\lambda_2 \\mid &lt; .1). Um erneut programmatisch die Items zu ermitteln, die anhand dieser Cutoff-Kriterium ausgeschlossen werden sollten, führen wir nochmal die Faktorenanalyse mit dem psych-Paket durch. Der Grund dafür liegt darin, dass das psych-Paket als Output der Funktion die Faktorladungen ausgibt, auf die wir mithilfe der loadings() Funktion zugreifen können. Danach können wir mit ein bisschen Logik mit der Funktion filter() aus dem dplyr-Paket alle Items filtern, die ausgeschlossen werden sollten.\n\nres_psych &lt;- psych::fa(\n  r = data_item_final, \n  nfactors = 2, # 2 Faktoren\n  fm = \"pa\" # Principal Axis Factoring\n  ) \n\ndf_fa &lt;- as.data.frame.matrix(loadings(res_psych))\ndf_fa$id_item &lt;- rownames(df_fa) # Neue Variable mit den Itemnamen\n\nstep_kick3_efa &lt;- df_fa %&gt;%\n  filter(\n    # 1. Kriterium\n    (PA1 &lt; .3 & PA2 &lt; .3) # Ladungen sowohl beim ersten als auch zweiten Faktor &lt;.3\n    | # ODER\n    # 2. Kriterium\n    (abs(PA1 - PA2) &lt; .1) # Absolute Differenz beider Faktoren &lt; .1\n    ) %&gt;%\n  pull(id_item)\n\n\n\n2.3.3 Finale Modellspezifizierung\nDie Faktorladungen von Item 13, Item 18 und Item 24 liegen unterhalb des festgelegten Cut-off-Kriteriums und wurden daher von der Filterfunktion als ausschlusswürdig identifiziert. Alle anderen Items laden weitestgehend gut auf einen Faktor und besitzen mittlere bis sehr hohe Faktorladungen. Für eine abschließende Modellspezifizierung erstellen wir einen neuen Datensatz ohne die drei genannten Variablen und führen erneut eine Faktorenanalyse durch.\n\ndata_item_final_s3_efa  &lt;- select(data_item_final, -all_of(step_kick3_efa))\n\ntab_fa(\n  data = data_item_final_s3_efa, \n  nmbr.fctr = 2,\n  rotation = \"oblimin\", \n  fctr.load.tlrn = 0, \n  method = \"pa\",\n  title = \"Finale Faktorenanalyse\"\n  )\n\n\nFinale Faktorenanalyse\n\n\n \nFactor 1\nFactor 2\n\n\nub_06\n-0.05\n0.72\n\n\nub_08\n0.02\n0.45\n\n\nub_09\n-0.07\n0.61\n\n\nub_10\n0.13\n0.33\n\n\nub_14_r\n0.99\n-0.04\n\n\nub_19\n0.21\n0.52\n\n\nub_20\n0.74\n-0.03\n\n\nub_27_r\n0.64\n0.23\n\n\nCronbach's α\n0.83\n0.67"
  },
  {
    "objectID": "script_efa.html#deskriptive-analyse-der-faktorstruktur",
    "href": "script_efa.html#deskriptive-analyse-der-faktorstruktur",
    "title": "2  EFA",
    "section": "2.4 Deskriptive Analyse der Faktorstruktur",
    "text": "2.4 Deskriptive Analyse der Faktorstruktur\nAnschließend können wir für jeden Faktor getrennt eine finale Itemanalyse durchführen. Das sjPlot Paket kann dabei alle zwei Tabellen für die Faktoren gleichzeitig in einer Funktion berechnen. Dafür muss zunächst jedoch ein Index erstellt werden, der spezifiziert, welche Items zu welchem Faktor gehören. Danach kann durch das Argument factor.groups die Faktorstruktur spezifiziert werden.\n\nfa_index &lt;- tab_fa(\n  data = data_item_final_s3_efa, \n  nmbr.fctr = 2, \n  rotation = \"oblimin\", \n  fctr.load.tlrn = 0, \n  method = \"pa\"\n  )$factor.index\n\nprint(fa_index)\n\n  ub_06   ub_08   ub_09   ub_10 ub_14_r   ub_19   ub_20 ub_27_r \n      2       2       2       2       1       2       1       1 \n\ntab_itemscale(\n  df = data_item_final_s3_efa, \n  factor.groups = fa_index, \n  factor.groups.titles = c(\"Faktor 1\", \"Faktor 2\") \n  )\n\n\nFaktor 1\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.80\n0.65\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.68\n0.79\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.62\n0.84\n\n\n\nMean inter-item-correlation=0.626 · Cronbach's α=0.834\n\n\n\n \n\nFaktor 2\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nα if deleted\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.52\n0.57\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.40\n0.63\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.47\n0.60\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.30\n0.67\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.44\n0.61\n\n\n\nMean inter-item-correlation=0.288 · Cronbach's α=0.670\n\n\n\n \n\n\n\n\n\n\n\n\n \nComponent 1\nComponent 2\n\n\nComponent 1\nα=0.834\n \n\n\nComponent 2\n0.313\n(.001)\nα=0.670\n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nEbenfalls sollte zusätzlich zur internen Konsistenz Cronbach´s \\alpha auch McDonald´s \\omega für jede Skala berechnet werden. Dafür erstellen wir zwei Variablen, die die Itemnamen für die beiden Skalen enthalten. Nun können wir eine kleine Funktion erstellen, mit der wir immer wieder für einen Datensatz \\alpha und \\omega berechnen können. Wir sparen uns somit die Arbeit, jedes Mal wieder den Code zu wiederholen, welcher im Body (hinter der eckigen Klammern) der Funktion steht. Wichtig ist, dass wir mit der return() angeben, was letztendlich dem User ausgegeben werden soll (sonst wird nichts angezeigt).\n\nf1_names &lt;- names(fa_index[fa_index == 1])\nf2_names &lt;- names(fa_index[fa_index == 2])\n\n\nget_reliability &lt;- function(.data, .var_names) {\n  fit_omega &lt;- omega(.data[, .var_names], plot = FALSE)\n  out &lt;- list(\n    omega = fit_omega$omega.tot, \n    alpha = fit_omega$alpha\n  )\n  # Angabe, welche Objekte ausgegeben werden sollen\n  return(out)\n}\n \nget_reliability(data_item_final_s3_efa, f1_names)\n\n$omega\n[1] 0.8538191\n\n$alpha\n[1] 0.8341717\n\nget_reliability(data_item_final_s3_efa, f2_names)\n\n$omega\n[1] 0.7115663\n\n$alpha\n[1] 0.6690035"
  },
  {
    "objectID": "script_efa.html#testwertanalyse",
    "href": "script_efa.html#testwertanalyse",
    "title": "2  EFA",
    "section": "2.5 Testwertanalyse",
    "text": "2.5 Testwertanalyse\nAls letztes werden wir die deskriptiven Kennwerte der Testwerte der drei Skalen genauer betrachten. Diese müssen wir zunächst erstmal berechnen. Dafür gibt es mehrere Möglichkeiten:\n\nMöglichkeit I - base RMöglichkeit II - tidyverse\n\n\n\ndata_item_final_scores &lt;- data_item_final_s3_efa\ndata_item_final_scores$f1 &lt;- rowSums(data_item_final_scores[, f1_names])\ndata_item_final_scores$f2 &lt;- rowSums(data_item_final_scores[, f2_names])\ndata_item_final_scores$ub_score &lt;- rowSums(data_item_final_scores)\n\ncolnames(data_item_final_scores)\n\n [1] \"ub_06\"    \"ub_08\"    \"ub_09\"    \"ub_10\"    \"ub_14_r\"  \"ub_19\"   \n [7] \"ub_20\"    \"ub_27_r\"  \"f1\"       \"f2\"       \"ub_score\"\n\n\n\n\nÜber die Funktion rowwise() wird angegeben, dass die nachfolgenden Operationen über die Zeilen (und nicht über die Spalten wie üblich) hinweg stattfinden sollen. Die Items, die aufsummiert werden, können über die Funktion c_across(variablen) angegeben werden.\n\ndata_item_final_scores &lt;- data_item_final_s3_efa %&gt;% \n  rowwise() %&gt;% \n  mutate(ub_score = sum(c_across(everything()))) %&gt;% \n  mutate(\n    f1 = sum(c_across(all_of(f1_names))),\n    f2 = sum(c_across(all_of(f2_names)))\n    ) \n\ncolnames(data_item_final_scores)\n\n [1] \"ub_06\"    \"ub_08\"    \"ub_09\"    \"ub_10\"    \"ub_14_r\"  \"ub_19\"   \n [7] \"ub_20\"    \"ub_27_r\"  \"ub_score\" \"f1\"       \"f2\"      \n\n\n\n\n\nWir können uns mittels der describe() des psych-Pakets, welche bereits im letzten Skript vorgestellt wurde die wichtigsten deskriptiven Kennwerte ausgeben lassen.\n\ntab_df(describe(data_item_final_scores$f1))\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n8.68\n3.50\n8\n8.63\n2.97\n0\n15\n15\n0.12\n-0.53\n0.35\n\n\n\n\n\n\ntab_df(describe(data_item_final_scores$f2))\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n12.32\n4.77\n13\n12.40\n4.45\n1\n24\n23\n-0.15\n-0.31\n0.47\n\n\n\n\n\n\n\nDiese deskriptiven Kennwerte können wir zudem abschließend in einem Histogramm mit unterlegter Normalverteilung visualisieren. Wir benutzen für das Plotten der Normalverteilung das Package ggh4x mit der Funktion stat_theodensity(). Das praktische hierbei ist, dass wir nicht aktiv die Parameter der Normalverteilung (M und SD) angeben müssen, sondern die Funktion das für uns im Hintergrund auf Basis der Daten berechnet. Im nachfolgenden Beispiel wird der Plot für den Gesamttestwert (alle Items) berechnet. Wenn eine der 3 Skalen geplottet werden soll, muss dies nur in der ggplot() Funktion angegeben werden (s.h. Code).\n\nlibrary(ggh4x)\n\nggplot(data_item_final_scores, aes(x = ub_score)) +  # oder x = f1/f2\n  geom_histogram(\n    binwidth = 3,\n    fill = \"lightgrey\",\n    ) +\n  stat_theodensity(\n    mapping = aes(y = after_stat(count)), \n    distri = \"norm\", \n    color = \"royalblue4\",\n    size = 0.5\n    ) +\n  scale_x_continuous(\n    name = \"Umweltbewusstsein\",\n    limits = c(0, 40), # Achsenlimit\n    breaks = seq(0, 40, 5), # Achsenabschnitte\n    expand = c(0, 0)\n    ) +\n  scale_y_continuous(\n    name = \"Häufigkeit\",\n    limits = c(0, 20),\n    breaks = seq(0, 20, 5),\n    expand = c(0, 0)\n  ) +\n  theme_sjplot()\n\n\n\n\n\n\n\n\nFür diejenigen unter euch, die sich in etwas anspruchvolleres R bzw. ggplot2 Territorium wagen wollen: Es gibt auch die Möglichkeit, alle Skalen gleichzeitig in einer Abbildung darzustellen. Um jeden einzelnen Schritt der großen Pipe zu verstehen, geht am besten jeden einzelnen Layer durch und schaut, wie sich der Plot verändert. Auch kann es hilfreich sein, wenn man die einzelnen Parameter in den Layern verändert, um zu schauen, was diese bewirken. Es reicht allerdings natürlich auch völlig aus, wenn ihr euch an dem letzten Plot orientiert!\n\n\nShow the code\nlibrary(ggh4x)\nlibrary(jcolors)\n\nstrip_labels &lt;- as_labeller(c(f1 = \"Factor 1\", f2 = \"Factor 2\", ub_score = \"Gesamtscore\"))\n\ndata_item_final_scores %&gt;% \n  # Variablen auswählen, die benötigt werden\n  select(f1, f2, ub_score) %&gt;% \n  # Vom Wide ins Long-Format\n  pivot_longer(\n    cols = everything(),\n    names_to = \"factor\",\n    values_to = \"score\"\n    ) %&gt;% \n  # ggplot Base Layer\n  ggplot(aes(x = score, fill = factor)) + \n  # facet_wrap erstellt kleineres Plotting Fenster anhand einer Variable\n  facet_wrap(\n    facets  = vars(factor), # Variable, welche gefacetted werden soll\n    labeller = strip_labels, # Facet Labels\n    scales = \"free\"  # freies Koordinatensystem\n    )  +\n  # Histogramm\n  geom_histogram(\n    bins = 15,\n    alpha = 0.6,\n    show.legend = FALSE,\n    aes(fill = factor)\n  ) +\n  # Theoretische Normalverteilung\n  stat_theodensity(aes(y = after_stat(count), color = factor)) +\n  # Achsenmanipulation in den einzelnen Facets\n  facetted_pos_scales(\n    x = list(\n      factor == \"f1\" ~ scale_x_continuous(limits = c(0, 25)),\n      factor == \"f2\" ~ scale_x_continuous(limits = c(0, 25)),\n      factor == \"ub_score\" ~ scale_x_continuous(limits = c(0, 40))\n      )\n    ) +\n  # Achsenmanipulation & Beschriftung\n  scale_y_continuous(\n    expand = c(0, 0),\n    limits = c(0, 40),\n    breaks = seq(0, 40, 10)\n    ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  xlab(\"Score\") + \n  ylab(\"Häufigkeit\") +\n  # Farben\n  scale_fill_manual(values = c(\"royalblue4\", \"orangered2\", \"ivory4\")) +\n  scale_color_manual(values = c(\"royalblue4\", \"orangered2\", \"ivory4\")) +\n  guides(color = \"none\", fill = \"none\") +\n  # Theme\n  theme_sjplot()"
  },
  {
    "objectID": "script_efa.html#session-info",
    "href": "script_efa.html#session-info",
    "title": "2  EFA",
    "section": "2.6 Session Info",
    "text": "2.6 Session Info\n\n\n\n\n\n\nErweitern für Session Info\n\n\n\n\n\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.1.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2023-11-27\n pandoc   3.1.4 @ /opt/homebrew/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package    * version date (UTC) lib source\n dplyr      * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n FactoMineR * 2.9     2023-10-12 [1] CRAN (R 4.3.1)\n forcats    * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggh4x      * 0.2.6   2023-08-30 [1] CRAN (R 4.3.0)\n ggplot2    * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n here       * 1.0.1   2020-12-13 [1] CRAN (R 4.3.0)\n jcolors    * 0.0.5   2023-09-26 [1] Github (jaredhuling/jcolors@81e72c8)\n lubridate  * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n psych      * 2.3.9   2023-09-26 [1] CRAN (R 4.3.1)\n purrr      * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr      * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sjPlot     * 2.8.15  2023-08-17 [1] CRAN (R 4.3.0)\n skimr      * 2.1.5   2022-12-23 [1] CRAN (R 4.3.0)\n stringr    * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble     * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr      * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse  * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────"
  }
]
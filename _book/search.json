[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PsyBSc8 Diagnostik - R Skript",
    "section": "",
    "text": "Einleitung\nThis is a Quarto book."
  },
  {
    "objectID": "script_itemanalysis.html#einleitung",
    "href": "script_itemanalysis.html#einleitung",
    "title": "1Â  Itemanalyse",
    "section": "1.1 Einleitung",
    "text": "1.1 Einleitung\nIm Rahmen dieses Tutorials zur Itemanalyse verwenden wir echte Daten aus einem im vergangenen Semester konstruierten Fragebogen zu Umweltbewusstsein, welcher die Einstellung von Personen gegenÃ¼ber dem Umweltschutz misst. Spezifisch wurde dieses Konstrukt auf Verhaltensebene operationalisiert, wobei mehr oder weniger umweltschÃ¼tzende Verhaltensweisen als Indikator fÃ¼r das AusmaÃŸ des umweltbewussten Handelns betrachtet werden. Neben den 29 Items der Rohfassung des Fragebogens enthÃ¤lt der Datensatz zusÃ¤tzlich demographische Informationen Ã¼ber das Alter, Geschlecht, Wohnsituation sowie Bildungsgrad der 104 teilnehmenden Personen. In der folgenden Tabelle erhaltet ihr eine Ãœbersicht Ã¼ber die Items des Konstrukts.\n\n\n\nÃœbersichtstabelle der Items zu Umweltbewusstsein\n\n\n\n\n\nub_01 ~ Ich trenne MÃ¼ll\n\n\nub_02 ~ Ich stecke nicht-benutzte ElektrogerÃ¤te aus\n\n\nub_03 ~ Ich nutze Strom aus erneuerbaren Energien\n\n\nub_04 ~ Ich nutze den Ã–PNV\n\n\nub_05 ~ Ich schalte das Licht in nicht-genutzten RÃ¤umen aus\n\n\nub_06 ~ Ich achte beim Einkauf von ElektrogerÃ¤ten auf Umweltsiegel\n\n\nub_07 ~ Wenn ich lÃ¼fte, mache ich die Heizung aus\n\n\nub_08 ~ Ich fahre sprit-sparend Auto\n\n\nub_09 ~ Ich achte beim Klamottenkauf auf kurze Lieferwege\n\n\nub_10 ~ Ich verkaufe/spende/ verschenke gebrauchte GegenstÃ¤nde\n\n\nub_11 ~ Ich beschreibe Vorder- und RÃ¼ckseite eines Blattes\n\n\nub_12 ~ Ich benutze Einmal-Hygiene Artikel\n\n\nub_13 ~ Bei Kurzstrecken laufe ich\n\n\nub_14 ~ Ich esse Fleisch\n\n\nub_15 ~ Ich kaufe regionale Lebensmittel\n\n\nub_16 ~ Ich kaufe unverpackte Lebensmittel\n\n\nub_17 ~ Ich werfe Lebensmittel weg\n\n\nub_18 ~ Ich kaufe Second-Hand-Kleidung\n\n\nub_19 ~ Ich achte beim Kauf von Hygieneartikeln auf biologisch abbaubare Inhaltsstoffe\n\n\nub_20 ~ Ich verzichte auf tierische Produkte\n\n\nub_21 ~ Ich nehme an Food-Sharing-Projekten teil\n\n\nub_22 ~ Ich lasse beim SpÃ¼len das Wasser laufen\n\n\nub_23 ~ An Ã¶ffentlichen Orten hinterlasse ich MÃ¼ll\n\n\nub_24 ~ Ich benutze wieder-verwendbare BehÃ¤ltnisse\n\n\nub_25 ~ Ich bewege mich bei Kurstrecken mit dem Fahrrad fort\n\n\nub_26 ~ Ich fliege Kurzstrecken\n\n\nub_27 ~ Ich esse Fleisch aus Massentierhaltung\n\n\nub_28 ~ Ich kaufe Second-Hand-MÃ¶bel\n\n\nub_29 ~ Ich fliege in den Urlaub"
  },
  {
    "objectID": "script_itemanalysis.html#einlesen-der-daten",
    "href": "script_itemanalysis.html#einlesen-der-daten",
    "title": "1Â  Itemanalyse",
    "section": "1.2 Einlesen der Daten",
    "text": "1.2 Einlesen der Daten\nDer Datensatz ist in diesem Fall als .rds-Datei gespeichert und kann mittels der Funktion (readRDS()) eingelesen werden.\n\n\n\n\n\n\nInformation zum Einlesen von Daten\n\n\n\nWenn ihr euren Datensatz aus SoSci-Survey herunterladet, wird dieser in einem anderen Format gespeichert sein (typischerweise entweder .csv, oder .xlsx). Um csv-Dateien (mit entweder -Komma, -Semikolon oder Tab-Separator) einzulesen, kÃ¶nnen entweder Base-R FunktionenÂ (read.csv(), read.csv2() bzw. read.delim()) oder z.B. Funktionen des readr-Packages aus der Familie des tidyverse genutzt werden (read_csv(), read_csv2() und read_tsv()). Um Excel-Dateien einlesen zu kÃ¶nnen, kann das readxl-Package genutzt werden (read_excel())\n\n\nIhr habt die MÃ¶glichkeit, den Datensatz entweder lokal von euren Computer einzulesen oder alternativ einen Permalink aus dem dazugehÃ¶rigen GitHub-Repository zu verwenden (dafÃ¼r ist natÃ¼rlich eine Internetverbindung erforderlich).\n\nMÃ¶glichkeit I: LokalMÃ¶glichkeit II: Link\n\n\nLadet den Datensatz aus OLAT herunter und stellt euer Working Directory richtig ein (setwd()). Danach kann der Datensatz folgendermaÃŸen eingelesen werden readRDS(\"data_ub_raw.rds\"). Alternativ wÃ¤re eine allgemeine Empfehlung R-Projects zu nutzen, dann mÃ¼sst ihr nicht mehr euer Working Directory einstellen und kÃ¶nnt relative Dateipfade nutzen readRDS(\"path/to/data/data_ub_raw.rds\"). Wer daran interessiert ist, findet unter diesem Link weitere Ressourcen.\n\nlibrary(here) # Bei Nutzung von R-Projects zur Nutzung von relativen Pfaden (statt setwd())\n\nhere() starts at /Users/luca/PowerFolders/Hiwi_Arbeit/DPPD/PsyBSc8_Diagnostik_WS2324\n\ndata_raw &lt;- readRDS(here::here(\"data/raw/data_ub_raw.rds\"))\n\n\n\nDurch Verwendung eines Permalinks kÃ¶nnen die Daten direkt von GitHub abgerufen werden. Dazu muss lediglich der Link als String in die Funktion url() eingegeben werden und kann anschlieÃŸend wie gewohnt eingelesen werden.\n\npermalink &lt;- \"https://github.com/jlschnatz/PsyBSc8_Diagnostik_WS2324/raw/95ba618424d0465f7e0bfea4cc5ac840d82ad74e/data/raw/data_ub_raw.rds\"\nif (curl::has_internet()) {\n  data_raw &lt;- readRDS(url(permalink))\n}\n\n\n\n\nWir laden den gesamten Datensatz und speichern zudem ein Subset des Datensatzes, welches nur die Items zu Umweltbewusstsein enthÃ¤lt. Hilfreiche Funktionen, um einen ersten Ãœberblick zu erhalten, sind zum Beispiel skim() oder str().\n\nlibrary(dplyr) # Datenmanipulation\nlibrary(skimr) # Hilfreich fÃ¼r Ãœberblick von Dataframes\n\ndata_item &lt;- data_raw %&gt;% dplyr::select(ub_01:ub_29) # oder select(starts_with(\"ub_\"))\ndata_item &lt;- subset(data_raw, select = ub_01:ub_29) # Alternative\n\n# Ãœberblick Ã¼ber den Datensatz\nskimr::skim(data_raw)\n\n\nData summary\n\n\nName\ndata_raw\n\n\nNumber of rows\n104\n\n\nNumber of columns\n35\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n5\n\n\nnumeric\n30\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nid_case\n0\n1.00\nFALSE\n104\n63: 1, 64: 1, 67: 1, 70: 1\n\n\ndd_sex\n3\n0.97\nFALSE\n2\nWei: 67, MÃ¤n: 34, Div: 0\n\n\ndd_wohn1\n3\n0.97\nFALSE\n4\nMit: 36, Mit: 28, In : 19, All: 18\n\n\ndd_wohn2\n3\n0.97\nFALSE\n4\nStÃ¤: 40, LÃ¤n: 27, Ehe: 21, Ehe: 13\n\n\ndd_bildung\n3\n0.97\nFALSE\n5\nAbi: 44, Hoc: 26, Ber: 17, Rea: 11\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ndd_alter\n3\n0.97\n31.27\n15.12\n14\n20\n24\n43\n81\nâ–‡â–‚â–‚â–‚â–\n\n\nub_01\n3\n0.97\n4.09\n0.86\n0\n4\n4\n5\n5\nâ–â–â–â–‡â–ƒ\n\n\nub_02\n3\n0.97\n2.50\n1.38\n0\n1\n2\n4\n5\nâ–‡â–…â–ƒâ–†â–‚\n\n\nub_03\n3\n0.97\n2.29\n1.34\n0\n1\n2\n3\n5\nâ–†â–‡â–†â–‚â–‚\n\n\nub_04\n3\n0.97\n2.56\n1.68\n0\n1\n3\n4\n5\nâ–‡â–ƒâ–…â–…â–ƒ\n\n\nub_05\n3\n0.97\n4.33\n0.81\n0\n4\n4\n5\n5\nâ–â–â–‚â–‡â–‡\n\n\nub_06\n3\n0.97\n2.69\n1.62\n0\n1\n3\n4\n5\nâ–‡â–ƒâ–†â–…â–…\n\n\nub_07\n3\n0.97\n3.26\n1.84\n0\n2\n4\n5\n5\nâ–†â–‚â–‚â–†â–‡\n\n\nub_08\n3\n0.97\n3.00\n1.51\n0\n2\n3\n4\n5\nâ–†â–†â–†â–‡â–†\n\n\nub_09\n3\n0.97\n1.45\n1.28\n0\n1\n1\n2\n5\nâ–‡â–ƒâ–‚â–â–\n\n\nub_10\n3\n0.97\n2.99\n1.36\n0\n2\n3\n4\n5\nâ–ƒâ–‡â–‡â–‡â–…\n\n\nub_11\n3\n0.97\n3.38\n1.37\n0\n3\n4\n4\n5\nâ–ƒâ–…â–‡â–‡â–‡\n\n\nub_12\n3\n0.97\n2.22\n1.40\n0\n1\n2\n3\n5\nâ–‡â–‡â–†â–‚â–‚\n\n\nub_13\n3\n0.97\n3.66\n1.28\n0\n3\n4\n5\n5\nâ–‚â–‚â–†â–†â–‡\n\n\nub_14\n3\n0.97\n2.05\n1.40\n0\n1\n2\n3\n5\nâ–‡â–†â–‡â–‚â–\n\n\nub_15\n3\n0.97\n2.86\n0.74\n1\n2\n3\n3\n5\nâ–â–ƒâ–‡â–‚â–\n\n\nub_16\n3\n0.97\n2.29\n0.86\n0\n2\n2\n3\n4\nâ–â–ƒâ–‡â–‡â–\n\n\nub_17\n3\n0.97\n3.44\n0.68\n2\n3\n3\n4\n5\nâ–‚â–‡â–â–‡â–\n\n\nub_18\n3\n0.97\n1.18\n1.15\n0\n0\n1\n2\n4\nâ–‡â–‡â–ƒâ–‚â–\n\n\nub_19\n3\n0.97\n2.19\n1.47\n0\n1\n2\n3\n5\nâ–‡â–…â–…â–‚â–‚\n\n\nub_20\n3\n0.97\n2.00\n1.43\n0\n1\n2\n3\n5\nâ–‡â–†â–ƒâ–â–‚\n\n\nub_21\n3\n0.97\n0.51\n0.89\n0\n0\n0\n1\n3\nâ–‡â–‚â–â–‚â–\n\n\nub_22\n3\n0.97\n1.21\n1.24\n0\n0\n1\n2\n5\nâ–‡â–ƒâ–â–â–\n\n\nub_23\n3\n0.97\n3.12\n0.35\n3\n3\n3\n3\n5\nâ–‡â–â–â–â–\n\n\nub_24\n3\n0.97\n3.78\n0.93\n0\n3\n4\n4\n5\nâ–â–â–…â–‡â–ƒ\n\n\nub_25\n3\n0.97\n2.20\n1.65\n0\n1\n2\n3\n5\nâ–‡â–†â–…â–‚â–ƒ\n\n\nub_26\n3\n0.97\n0.33\n0.81\n0\n0\n0\n0\n5\nâ–‡â–â–â–â–\n\n\nub_27\n3\n0.97\n1.27\n1.19\n0\n0\n1\n2\n5\nâ–‡â–ƒâ–â–â–\n\n\nub_28\n3\n0.97\n1.17\n1.18\n0\n0\n1\n2\n4\nâ–‡â–†â–…â–‚â–\n\n\nub_29\n3\n0.97\n1.58\n1.13\n0\n1\n2\n2\n5\nâ–‡â–†â–‚â–â–\n\n\n\n\n\nBevor wir mit der deskriptiven Analyse beginnen, sollten wir noch Ã¼berprÃ¼fen, ob es fehlende Werte (NAs) im Datensatz gibt.\n\nanyNA(data_item)\n\n[1] TRUE\n\nsum(is.na(data_item)) # Alternative\n\n[1] 87\n\n\nIn diesem Fall sind 87 fehlende Werte vorhanden. Um die fehlenden Werte genauer zu explorieren, sollte am Rande das naniar-Package erwÃ¤hnt werden, da es einige nÃ¼tzliche Funktionen diesbezÃ¼glich enthÃ¤lt. Wir kÃ¶nnen zum Beispiel Ã¼ber die cases (Proband:innen) hinweg die prozentuale HÃ¤ufigkeit an Missings visualisieren.\n\nnaniar::gg_miss_case(data_item, show_pct = TRUE, order_cases = FALSE)\n\n\n\n\n\n\n\n\nWir sehen, dass drei Proband:innen den kompletten Fragebogen nicht ausgefÃ¼llt haben, weswegen mit der Funktion na.omit() ausgeschlossen werden.\n\n\n\n\n\n\nDisclaimer zu fehlenden Werten\n\n\n\nEs sollte zumindest am Rande erwÃ¤hnt werden, dass die Funktion na.omit() sehr mÃ¤chtig ist und nur mit Vorsicht verwendet werden sollte. Ohne weitere Ãœberlegungen NAs auszuschlieÃŸen, deren Fehlen mÃ¶glicherweise nicht zufÃ¤llig ist, sondern durch andere (nicht)-erhobene Variablen bedingt sind, kann zu Verzerrungen fÃ¼hren. Im Zweifel sprecht ihr euch am besten mit eurer/eurem Dozent:in ab, wie ihr mit fehlenden Werten umgehen sollt. Falls ihr euch mehr mit dem Thema NAs auseinandersetzen wollt, hier ein spannender Blogpost.\n\n\n\ndata_item &lt;- na.omit(data_item)\nanyNA(data_item)\n\n[1] FALSE"
  },
  {
    "objectID": "script_itemanalysis.html#deskriptive-analyse",
    "href": "script_itemanalysis.html#deskriptive-analyse",
    "title": "1Â  Itemanalyse",
    "section": "1.3 Deskriptive Analyse",
    "text": "1.3 Deskriptive Analyse\nBevor wir die Itemanalyse durchfÃ¼hren, wollen wir uns zunÃ¤chst ein wenig mit den Daten vertraut machen. HierfÃ¼r benÃ¶tigen wir zwei Packages: Das psych-Package beinhaltet sehr viele Funktionen und Befehle, die auch fÃ¼r viele andere Analysen hilfreich sind. Das Package janitor ist eine bessere Alternative zum Basis-Befehl table() und ist besonders informativ bei HÃ¤ufigkeitstabellen.\n\n\n\n\n\n\nTipp fÃ¼r die PrÃ¤sentationen & Hausarbeit\n\n\n\nFÃ¼r die Abschlussberichte, braucht ihr die ganzen deskriptiven Informationen in APA7 formatierten Tabellen. HierfÃ¼r eignet sich besonders das Package sjPlot. Als Beispiel speichern wir zunÃ¤chst die die vorherige deskriptive Statistik aller Items als ein Objekt ab. Danach erstellen wir mit einer Funktion des genannten Packages eine schÃ¶n formatierte Tabelle. Die erstellte Tabelle kann sogar direkt als Word-Dokument abgespeichert werden, um danach noch weiter angepasst zu werden (z.B. Erstellen von FuÃŸnoten, Tabellen-Titel, etc.). Wichtig dabei ist, dass nur die Endung .doc und nicht .docx funktioniert.\n\n\nFÃ¼r die Berechnung deskriptiver Kennwerte (Mittelwert, Standardabweichung, Median, etc.) kÃ¶nnen wir die describe() Funktion des psych-Packages verwenden:\n\nlibrary(psych)\nlibrary(sjPlot)\ndescr_data_item &lt;- psych::describe(data_item)\nsjPlot::tab_df(\n  x = descr_data_item, \n  show.rownames = TRUE,\n  #file = table_descr_item.doc # Speichern als .doc Datei\n    )\n\n\n\n\nRow\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\nub_01\n1\n101\n4.09\n0.86\n4\n4.21\n0.00\n0\n5\n5\n-2.21\n8.27\n0.09\n\n\nub_02\n2\n101\n2.50\n1.38\n2\n2.46\n1.48\n0\n5\n5\n0.13\n-1.27\n0.14\n\n\nub_03\n3\n101\n2.29\n1.34\n2\n2.26\n1.48\n0\n5\n5\n0.23\n-0.46\n0.13\n\n\nub_04\n4\n101\n2.56\n1.68\n3\n2.58\n1.48\n0\n5\n5\n-0.05\n-1.29\n0.17\n\n\nub_05\n5\n101\n4.33\n0.81\n4\n4.46\n1.48\n0\n5\n5\n-1.86\n6.44\n0.08\n\n\nub_06\n6\n101\n2.69\n1.62\n3\n2.74\n1.48\n0\n5\n5\n-0.11\n-1.22\n0.16\n\n\nub_07\n7\n101\n3.26\n1.84\n4\n3.44\n1.48\n0\n5\n5\n-0.70\n-1.04\n0.18\n\n\nub_08\n8\n101\n3.00\n1.51\n3\n3.10\n1.48\n0\n5\n5\n-0.41\n-0.83\n0.15\n\n\nub_09\n9\n101\n1.45\n1.28\n1\n1.30\n1.48\n0\n5\n5\n0.93\n0.52\n0.13\n\n\nub_10\n10\n101\n2.99\n1.36\n3\n3.06\n1.48\n0\n5\n5\n-0.34\n-0.50\n0.14\n\n\nub_11\n11\n101\n3.38\n1.37\n4\n3.52\n1.48\n0\n5\n5\n-0.62\n-0.35\n0.14\n\n\nub_12\n12\n101\n2.22\n1.40\n2\n2.16\n1.48\n0\n5\n5\n0.27\n-0.48\n0.14\n\n\nub_13\n13\n101\n3.66\n1.28\n4\n3.81\n1.48\n0\n5\n5\n-0.85\n0.24\n0.13\n\n\nub_14\n14\n101\n2.05\n1.40\n2\n2.01\n1.48\n0\n5\n5\n-0.04\n-0.84\n0.14\n\n\nub_15\n15\n101\n2.86\n0.74\n3\n2.83\n0.00\n1\n5\n4\n0.22\n0.48\n0.07\n\n\nub_16\n16\n101\n2.29\n0.86\n2\n2.32\n1.48\n0\n4\n4\n-0.30\n-0.30\n0.09\n\n\nub_17\n17\n101\n3.44\n0.68\n3\n3.48\n1.48\n2\n5\n3\n-0.23\n-0.37\n0.07\n\n\nub_18\n18\n101\n1.18\n1.15\n1\n1.02\n1.48\n0\n4\n4\n0.90\n0.03\n0.11\n\n\nub_19\n19\n101\n2.19\n1.47\n2\n2.14\n1.48\n0\n5\n5\n0.23\n-0.89\n0.15\n\n\nub_20\n20\n101\n2.00\n1.43\n2\n1.90\n1.48\n0\n5\n5\n0.43\n-0.49\n0.14\n\n\nub_21\n21\n101\n0.51\n0.89\n0\n0.33\n0.00\n0\n3\n3\n1.51\n1.02\n0.09\n\n\nub_22\n22\n101\n1.21\n1.24\n1\n1.02\n1.48\n0\n5\n5\n0.93\n0.24\n0.12\n\n\nub_23\n23\n101\n3.12\n0.35\n3\n3.01\n0.00\n3\n5\n2\n2.96\n8.63\n0.04\n\n\nub_24\n24\n101\n3.78\n0.93\n4\n3.85\n1.48\n0\n5\n5\n-0.80\n1.34\n0.09\n\n\nub_25\n25\n101\n2.20\n1.65\n2\n2.12\n1.48\n0\n5\n5\n0.22\n-1.05\n0.16\n\n\nub_26\n26\n101\n0.33\n0.81\n0\n0.14\n0.00\n0\n5\n5\n3.43\n13.66\n0.08\n\n\nub_27\n27\n101\n1.27\n1.19\n1\n1.12\n1.48\n0\n5\n5\n0.96\n0.95\n0.12\n\n\nub_28\n28\n101\n1.17\n1.18\n1\n1.02\n1.48\n0\n4\n4\n0.74\n-0.38\n0.12\n\n\nub_29\n29\n101\n1.58\n1.13\n2\n1.51\n1.48\n0\n5\n5\n0.73\n0.79\n0.11\n\n\n\n\n\n\n\nWenn wir nur eine spezifische Variable deskriptiv betrachten wollen (z.B. dd_alter), kann in der gleichen Funktion die Variable direkt angesteuert werden.\n\ndescr_age &lt;- psych::describe(data_raw$dd_alter)\nsjPlot::tab_df(descr_age)\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n31.27\n15.12\n24\n29.28\n7.41\n14\n81\n67\n1.07\n-0.02\n1.50\n\n\n\n\n\n\n\nFÃ¼r alle kategoriellen Daten (z.B. Geschlecht, Wohnort, Bildung) benÃ¶tigen keine Mittelwerte oder Standardabweichungen, sondern nutzen HÃ¤ufigkeitsverteilung zur deskriptiven Beschreibung. Hier kommt jetzt das janitor-Package zum Einsatz.\n\nlibrary(janitor)\ndescr_sex &lt;- tabyl(data_raw$dd_sex, show_na = FALSE)\ntab_df(descr_sex)\n\n\n\n\ndata_raw.dd_sex\nn\npercent\n\n\nMÃ¤nnlich\n34\n0.34\n\n\nWeiblich\n67\n0.66\n\n\nDivers\n0\n0.00\n\n\n\n\n\n\n\nWir bekommen die relativen und absoluten HÃ¤ufigkeiten fÃ¼r mÃ¤nnliche und weibliche Probanden ausgegeben. Falls es fehlenden Werte gÃ¤be, mÃ¼ssten diese im Bericht auch angegeben werden. Dies ist ebenfalls mit der gleichen Funktion durch die Spezifizierung eines zusÃ¤tzlichen Arguments mÃ¶glich.\n\ndescr_sex_na &lt;- tabyl(data_raw$dd_sex, show_na = TRUE)\ntab_df(descr_sex_na)\n\n\n\n\ndata_raw.dd_sex\nn\npercent\nvalid_percent\n\n\nMÃ¤nnlich\n34\n0.33\n0.34\n\n\nWeiblich\n67\n0.64\n0.66\n\n\nDivers\n0\n0.00\n0.00\n\n\nNA\n3\n0.03\nNA\n\n\n\n\n\n\n\nZudem kÃ¶nnen wir mit dem psych-Package auch eine Tabelle nach Gruppen erstellen. Dieser Output kann dann mit einer Ã¤hnlichen Funktion des sjPlot-Package in einer Tabelle dargestellt werden.\n\ndescr_age_by_sex &lt;- describeBy(\n  x = data_raw$dd_alter, group = droplevels(data_raw$dd_sex) # Nicht angegebenes Level (Divers) des Faktors entfernen\n  )\n\ntab_dfs(\n  x = descr_age_by_sex,\n  titles = c(\"MÃ¤nnlich\",\"Weiblich\")\n  )\n\n\nMÃ¤nnlich\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n34\n26.65\n10.20\n23.50\n24.93\n4.45\n14\n58\n44\n1.72\n2.57\n1.75\n\n\n\nÂ \n\nWeiblich\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n67\n33.61\n16.67\n25\n32\n8.90\n14\n81\n67\n0.75\n-0.76\n2.04\n\n\n\n\n\n\n\nEs gibt auch die MÃ¶glichkeit mehrere Tabellen in ein Dokument zu packen und diese in einem Word-Dokument zu speichern:\n\ntab_dfs(\n  x = list(descr_age, descr_sex_na),\n  titles = c(\"Descriptives of Age\", \"Descriptives of Sex\")\n  )\n\n\nDescriptives of Age\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n31.27\n15.12\n24\n29.28\n7.41\n14\n81\n67\n1.07\n-0.02\n1.50\n\n\n\nÂ \n\nDescriptives of Sex\n\n\ndata_raw$dd_sex\nn\npercent\nvalid_percent\n\n\nMÃ¤nnlich\n34\n0.33\n0.34\n\n\nWeiblich\n67\n0.64\n0.66\n\n\nDivers\n0\n0.00\n0.00\n\n\nNA\n3\n0.03\nNA\n\n\n\n\n\n\n\n\ntab_dfs(\n  x = list(descr_age, descr_sex),\n  titles = c(\"Descriptives of Age\",\"Descriptives of Sex\"),\n  file = \"descriptives_all.doc\" # wieder als .doc abspeichern\n  )"
  },
  {
    "objectID": "script_itemanalysis.html#itemanalyse",
    "href": "script_itemanalysis.html#itemanalyse",
    "title": "1Â  Itemanalyse",
    "section": "1.4 Itemanalyse",
    "text": "1.4 Itemanalyse\n\n1.4.1 Rekodierung inverser Items\nFÃ¼r die Itemanalyse benÃ¶tigen wir den Datensatz in denen nur die Items vorhanden sind. Diesen haben wir bereits in einem vorherigen Schritt erstellt. Bevor wir die Itemanalyse jedoch durchfÃ¼hren, mÃ¼ssen wir alle Items, die inverse kodiert sind rekodieren.\n\n\n\n\n\n\nRekodierung leicht gemacht!\n\n\n\nDafÃ¼r speichern wir alle inversen Items zunÃ¤chst in einem Vektor ab. AnschlieÃŸend verwenden wir die mutate() Funktion des dplyr-Package, mit welcher wir Variablen manupulieren/verÃ¤ndern kÃ¶nnen. Wir mÃ¼ssen dabei den Zusatz across() hinzunehmen, da wir mehreren Variablen gleichzeitig verÃ¤ndern wollen. Das Argument .cols gibt dabei an, welche Variablen wir verÃ¤ndern wollen. Mit dem Argument .fns spezifizieren wir, welche Funktion wir auf die Variablen anwenden wollen. Wir verwenden die Funktion rec() aus dem sjmisc-Package. Die etwas ungewÃ¶hnliche Schreibweise mit der Tilde ~und dem .x setzt sich wie folgt zusammen: Die Tilde mÃ¼ssen wir immer dann verwenden, wenn wir bei der Funktion, die wir anwenden zusÃ¤tzlich Argumente spezifizieren (rec = \"rev\"). Das .x verwenden wir als Platzhalter fÃ¼r alle Variablen, die wir verÃ¤ndern wollen. SchlieÃŸlich kÃ¶nnen wir mit dem .names Argument einen Namen fÃ¼r alle verÃ¤nderten Variablen spezifizieren. Das Prefix {col} steht dabei fÃ¼r den ursprÃ¼nglichen Variablenname. Mit dem Zusatz {col_r} wird hÃ¤ngen wir dem PrÃ¤fix noch ein Suffix an. Das Suffix kennzeichnet dabei, dass wir die Items rekodiert haben.\n\n\n\nlibrary(sjmisc)\ninverse_items &lt;- paste0(\"ub_\", c(12, 14, 17, 22, 23, 26, 27, 29))\n\ndata_item_rec &lt;- data_item %&gt;%\n  mutate(across(\n    .cols = all_of(inverse_items), \n    .fns = ~sjmisc::rec(.x, rec = \"rev\"),\n    .names = \"{.col}_r\"\n    )) %&gt;%\n  select(-all_of(inverse_items))\n\ncolnames(data_item_rec)\n\n [1] \"ub_01\"   \"ub_02\"   \"ub_03\"   \"ub_04\"   \"ub_05\"   \"ub_06\"   \"ub_07\"  \n [8] \"ub_08\"   \"ub_09\"   \"ub_10\"   \"ub_11\"   \"ub_13\"   \"ub_15\"   \"ub_16\"  \n[15] \"ub_18\"   \"ub_19\"   \"ub_20\"   \"ub_21\"   \"ub_24\"   \"ub_25\"   \"ub_28\"  \n[22] \"ub_12_r\" \"ub_14_r\" \"ub_17_r\" \"ub_22_r\" \"ub_23_r\" \"ub_26_r\" \"ub_27_r\"\n[29] \"ub_29_r\"\n\n\nWir sehen, dass die Variablen jetzt nicht mehr in der â€œrichtigenâ€ Reihenfolge sind, da alle rekodierten Items am Ende des Dataframes auftauchen. Wir kÃ¶nnen die ursprÃ¼ngliche Reihenfolge der Items wiederherstellen, indem wir diese in einem Vektor spezifizieren. AnschlieÃŸend verwenden wir wieder die select() Funktion und bringen dadurch die Variablen in die gewÃ¼nschte Reihenfolge.\n\ncol_order &lt;- sort(colnames(data_item_rec))\ndata_item_rec &lt;- select(data_item_rec, all_of(col_order))\ncolnames(data_item_rec)\n\n [1] \"ub_01\"   \"ub_02\"   \"ub_03\"   \"ub_04\"   \"ub_05\"   \"ub_06\"   \"ub_07\"  \n [8] \"ub_08\"   \"ub_09\"   \"ub_10\"   \"ub_11\"   \"ub_12_r\" \"ub_13\"   \"ub_14_r\"\n[15] \"ub_15\"   \"ub_16\"   \"ub_17_r\" \"ub_18\"   \"ub_19\"   \"ub_20\"   \"ub_21\"  \n[22] \"ub_22_r\" \"ub_23_r\" \"ub_24\"   \"ub_25\"   \"ub_26_r\" \"ub_27_r\" \"ub_28\"  \n[29] \"ub_29_r\"\n\n\n\n\n1.4.2 Itemanalyse I: Schwierigkeit\nJetzt kÃ¶nnen wir die Itemanalyse durchfÃ¼hren. Wir verwenden dafÃ¼r eine Funktion aus dem sjPlot-Package:\n\n\n\n\n\n\nSchritt I\n\n\n\nIm ersten Schritt der Itemanalyse schauen wir uns die Schwierigkeiten der Items an und schlieÃŸen Items aus, die zu schwer beziehungsweise zu leicht sind. Eine Faustregel, die auch in den PrÃ¤sentationsfolien zur Itemanalyse erwÃ¤hnt wurde, besagt, dass wir Items beibehalten sollten, bei denen 0.2 &lt; P_i &lt; 0.8 gilt. Dabei steht P_i fÃ¼r die Itemschwierigkeit. Von dieser Faustregel kann jedoch auch abgewichen werden bei der Erfassung von Konstrukten mit extremer Streuung oder wenn gezielt besonders schwere oder leichte Items im Konstrukt enthalten sein sollen (.05 &lt; P_i &lt; 0.95)\n\n\n\nitem_analysis_1 &lt;- tab_itemscale(\n  df = data_item_rec,\n  factor.groups.titles = \"Erste Itemanalyse\"\n  )\n\nitem_analysis_1\n\n\nErste Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nub_01\n0.00 %\n4.09\n0.86\n-2.28\n0.82\n0.33\n0.75\n\n\n\nub_02\n0.00 %\n2.5\n1.38\n0.14\n0.50\n0.25\n0.75\n\n\n\nub_03\n0.00 %\n2.29\n1.34\n0.24\n0.46\n0.24\n0.75\n\n\n\nub_04\n0.00 %\n2.56\n1.68\n-0.05\n0.51\n0.15\n0.76\n\n\n\nub_05\n0.00 %\n4.33\n0.81\n-1.92\n0.87\n0.17\n0.75\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.35\n0.74\n\n\n\nub_07\n0.00 %\n3.26\n1.84\n-0.72\n0.65\n0.28\n0.75\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.38\n0.74\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.32\n0.74\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.37\n0.74\n\n\n\nub_11\n0.00 %\n3.38\n1.37\n-0.64\n0.68\n0.28\n0.75\n\n\n\nub_12_r\n0.00 %\n2.78\n1.4\n-0.27\n0.56\n0.09\n0.76\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.43\n0.74\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.50\n0.73\n\n\n\nub_15\n0.00 %\n2.86\n0.74\n0.22\n0.57\n0.15\n0.75\n\n\n\nub_16\n0.00 %\n2.29\n0.86\n-0.31\n0.57\n0.26\n0.75\n\n\n\nub_17_r\n0.00 %\n3.56\n0.68\n0.24\n0.71\n0.21\n0.75\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.30\n0.75\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.50\n0.73\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.33\n0.74\n\n\n\nub_21\n0.00 %\n0.51\n0.89\n1.56\n0.17\n0.22\n0.75\n\n\n\nub_22_r\n0.00 %\n3.79\n1.24\n-0.96\n0.76\n0.29\n0.75\n\n\n\nub_23_r\n0.00 %\n4.88\n0.35\n-3.05\n0.98\n0.31\n0.75\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.39\n0.74\n\n\n\nub_25\n0.00 %\n2.2\n1.65\n0.22\n0.44\n0.12\n0.76\n\n\n\nub_26_r\n0.00 %\n4.67\n0.81\n-3.53\n0.93\n0.10\n0.76\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.46\n0.74\n\n\n\nub_28\n0.00 %\n1.17\n1.18\n0.76\n0.29\n0.18\n0.75\n\n\n\nub_29_r\n0.00 %\n3.42\n1.13\n-0.75\n0.68\n0.00\n0.76\n\n\n\nMean inter-item-correlation=0.100 Â· Cronbach's Î±=0.755\n\n\n\n\n\n\n\nIn dieser Situation entscheiden wir uns dazu, nur diejenigen Items zu behalten, fÃ¼r die 0.2 &lt; P_i &lt; 0.8 gilt. Es gibt zwei Methoden, dies umzusetzen: Eine MÃ¶glichkeit besteht darin, den Output manuell zu Ã¼berprÃ¼fen, um diejenigen Items zu identifizieren, die dieses Kriterium nicht erfÃ¼llen. Diese Items werden dann in einem Vektor gespeichert und anschlieÃŸend ausgeschlossen. Eine andere, etwas elegantere LÃ¶sung, die programmatischer vorgeht, besteht darin, direkt in R diejenigen Items herauszufiltern, die gemÃ¤ÃŸ diesem Kriterium ausgeschlossen werden sollen. DafÃ¼r machen wir uns zu Nutzen, dass die Funktion sjPlot::tab_itemscale() uns nicht nur eine schÃ¶ne HTML-Tabelle ausgibt, sondern diese im Hintergrund auch mit ausgibt. Durch die Verwendung von der Funktion str() kÃ¶nnen wir genauer betrachten, welche Informationen im Hintergrund ausgegeben werden, wenn wir das Objekt item_analysis_1 aufrufen.\n\nstr(object = item_analysis_1, vec.len = 1, nchar.max = 30)\n\nList of 10\n $ page.style     : chr \"&lt;style&gt;\\nhtml\"| __truncated__\n $ page.content   : chr \"&lt;table&gt;\\n&lt;cap\"| __truncated__\n $ page.complete  : chr \"&lt;html&gt;\\n&lt;head\"| __truncated__\n $ knitr          : chr \"&lt;table style=\"| __truncated__\n $ file           : NULL\n $ viewer         : logi TRUE\n $ df.list        :List of 1\n  ..$ :'data.frame':    29 obs. of  7 variables:\n  .. ..$ Missings           : chr [1:29] \"0.00 %\" ...\n  .. ..$ Mean               : chr [1:29] \"4.09\" ...\n  .. ..$ SD                 : chr [1:29] \"0.86\" ...\n  .. ..$ Skew               : chr [1:29] \"-2.28\" ...\n  .. ..$ Item Difficulty    : num [1:29] 0.82 0.5 ...\n  .. ..$ Item Discrimination: num [1:29] 0.328 0.248 ...\n  .. ..$ &alpha; if deleted : num [1:29] 0.746 0.749 ...\n $ index.scores   :'data.frame':    101 obs. of  1 variable:\n  ..$ Score1: num [1:101] 2.69 2.69 ...\n  .. ..- attr(*, \"label\")= Named chr \"Mean icc=0.10\"| __truncated__\n  .. .. ..- attr(*, \"names\")= chr \"Score1\"\n $ cronbach.values:List of 1\n  ..$ : num 0.755\n $ ideal.item.diff:List of 1\n  ..$ : Named num [1:29] 0.6 0.6 ...\n  .. ..- attr(*, \"names\")= chr [1:29] \"ub_01\" ...\n - attr(*, \"class\")= chr \"sjTable\"\n\n\nWie ersichtlich ist, ist der Output als Liste strukturiert und wir benÃ¶tigen den Eintrag df.list. Danach mÃ¼ssen wir erneut in die Liste indizieren (item_analysis_1$df.list[[1]]). Da wir in diesem Skript mehrmals auf genau diese Itemtabelle zugreifen mÃ¶chten, ist es ratsam, eine kleine Funktion zu schreiben, die diese Aufgabe Ã¼bernimmt.\n\nextract_itemtable &lt;- function(.data) {\n  tab &lt;- sjPlot::tab_itemscale(.data)$df.list[[1]] \n  # clean_names Ã¤ndert die Spaltennamen in lowercase und tauscht Leerzeichen mit _\n  out &lt;- janitor::clean_names(cbind(id_item = rownames(tab), tab))\n  return(out)\n}\n\n\nhead(extract_itemtable(data_item_rec))\n\n      id_item missings mean   sd  skew item_difficulty item_discrimination\nub_01   ub_01   0.00 % 4.09 0.86 -2.28            0.82               0.328\nub_02   ub_02   0.00 %  2.5 1.38  0.14            0.50               0.248\nub_03   ub_03   0.00 % 2.29 1.34  0.24            0.46               0.243\nub_04   ub_04   0.00 % 2.56 1.68 -0.05            0.51               0.153\nub_05   ub_05   0.00 % 4.33 0.81 -1.92            0.87               0.166\nub_06   ub_06   0.00 % 2.69 1.62 -0.11            0.54               0.349\n      alpha_if_deleted\nub_01            0.746\nub_02            0.749\nub_03            0.750\nub_04            0.758\nub_05            0.753\nub_06            0.743\n\n\nJetzt kÃ¶nnen wir die filter() Funktion des dplyr-Packages nutzen, um die Zeilen zu ermitteln, die in das Auschlusskriterium fallen.\n\nstep1_kick_diff &lt;- extract_itemtable(data_item_rec) %&gt;%\n  filter(item_difficulty &lt; .2 | item_difficulty &gt; .8) %&gt;% # `|` ist der ODER Operator in R \n  dplyr::pull(id_item) # macht das selbe wie `$id_item`(indiziert in den dataframe)\n\nprint(step1_kick_diff)\n\n[1] \"ub_01\"   \"ub_05\"   \"ub_21\"   \"ub_23_r\" \"ub_26_r\"\n\n\nAnschlieÃŸend kÃ¶nnen wir ein neues Objekt erstellen, indem wir die 5 Items aus step1_kick_diff ausgeschlossen haben.\n\ndata_item_s1 &lt;- dplyr::select(data_item_rec, -all_of(step1_kick_diff))\n# data_item_s1 &lt;- data_item_rec[, !colnames(data_item_rec) %in% step1_kick_diff] # Alternative\n\n\n\n1.4.3 Itemanalyse II: TrennschÃ¤rfe\nNachdem wir in der ersten Itemanalyse hinsichtlich der Itemschwierigkeit fÃ¼nf Items ausgeschlossen haben, folgt die zweite Itemanalyse hinsichtlich der ItemtrennschÃ¤rfe.\n\n\n\n\n\n\nSchritt II\n\n\n\nAls nÃ¤chsten Schritt der Itemanalyse widmen wir uns nun der TrennschÃ¤rfe der Item, also der Ãœbereinstimmung der DifferenzierungsfÃ¤higkeit eines Items mit dem Testscore der restlichen Items des Fragebogens. Dabei gilt die Faustregel, dass idealerweise ğ‘Ÿ_{ğ‘–ğ‘¡(ğ‘–)} \\geq .4 sein sollte. Nach einer sorgfÃ¤ltigen ÃœberprÃ¼fung kÃ¶nnen jedoch auch Items beibehalten werden, bei denen ğ‘Ÿ_{ğ‘–ğ‘¡(ğ‘–)} \\geq .3 liegt.\n\n\nIn dieser Situation entscheiden wir uns das liberalere Kriterium von ğ‘Ÿ_{ğ‘–ğ‘¡(ğ‘–)} \\geq .3 zu wÃ¤hlen.\n\nitem_analysis_2 &lt;- tab_itemscale(\n  df = data_item_s1,\n  factor.groups.titles = \"Zweite Itemanalyse\"\n  )\n\nitem_analysis_2\n\n\nZweite Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nub_02\n0.00 %\n2.5\n1.38\n0.14\n0.50\n0.23\n0.73\n\n\n\nub_03\n0.00 %\n2.29\n1.34\n0.24\n0.46\n0.21\n0.73\n\n\n\nub_04\n0.00 %\n2.56\n1.68\n-0.05\n0.51\n0.19\n0.74\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.33\n0.72\n\n\n\nub_07\n0.00 %\n3.26\n1.84\n-0.72\n0.65\n0.26\n0.73\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.37\n0.72\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.32\n0.73\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.38\n0.72\n\n\n\nub_11\n0.00 %\n3.38\n1.37\n-0.64\n0.68\n0.26\n0.73\n\n\n\nub_12_r\n0.00 %\n2.78\n1.4\n-0.27\n0.56\n0.05\n0.75\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.47\n0.71\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.51\n0.71\n\n\n\nub_15\n0.00 %\n2.86\n0.74\n0.22\n0.57\n0.14\n0.74\n\n\n\nub_16\n0.00 %\n2.29\n0.86\n-0.31\n0.57\n0.25\n0.73\n\n\n\nub_17_r\n0.00 %\n3.56\n0.68\n0.24\n0.71\n0.18\n0.73\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.33\n0.73\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.49\n0.71\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.34\n0.72\n\n\n\nub_22_r\n0.00 %\n3.79\n1.24\n-0.96\n0.76\n0.29\n0.73\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.40\n0.72\n\n\n\nub_25\n0.00 %\n2.2\n1.65\n0.22\n0.44\n0.14\n0.74\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.46\n0.72\n\n\n\nub_28\n0.00 %\n1.17\n1.18\n0.76\n0.29\n0.20\n0.73\n\n\n\nub_29_r\n0.00 %\n3.42\n1.13\n-0.75\n0.68\n-0.04\n0.75\n\n\n\nMean inter-item-correlation=0.107 Â· Cronbach's Î±=0.737\n\n\n\n\n\n\n\nWir greifen erneut auf die zuvor definierte Funktion extract_itemtable() zurÃ¼ck, um die erforderlichen Informationen zu extrahieren. Diesmal filtern wir nach der Variable item_discrimination. Den resultierenden Vektor speichern wir erneut als Objekt und nutzen ihn zur Erstellung eines endgÃ¼ltigen Datensatzes, der nur noch Variablen enthÃ¤lt, die sowohl angemessene Schwierigkeit als auch TrennschÃ¤rfe aufweisen.\n\nstep2_kick_disc &lt;- extract_itemtable(data_item_s1) %&gt;%\n  dplyr::filter(item_discrimination &lt; .3) %&gt;%\n  dplyr::pull(id_item)\n\nprint(step2_kick_disc)\n\n [1] \"ub_02\"   \"ub_03\"   \"ub_04\"   \"ub_07\"   \"ub_11\"   \"ub_12_r\" \"ub_15\"  \n [8] \"ub_16\"   \"ub_17_r\" \"ub_22_r\" \"ub_25\"   \"ub_28\"   \"ub_29_r\"\n\ndata_item_s2 &lt;- dplyr::select(data_item_s1, -all_of(step2_kick_disc))\n\nMit diesem Datensatz kÃ¶nnen wir nun die finale Itemanalyse durchfÃ¼hren.\n\ntab_itemscale(\n  df = data_item_s2,\n  factor.groups.titles = \"Finale Itemanalyse\"\n  )\n\n\nFinale Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.41\n0.74\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.36\n0.75\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.35\n0.75\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.38\n0.74\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.35\n0.75\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.55\n0.72\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.23\n0.76\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.51\n0.72\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.43\n0.74\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.35\n0.75\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.56\n0.72\n\n\n\nMean inter-item-correlation=0.222 Â· Cronbach's Î±=0.758\n\n\n\n\n\n\n\n\n\n\n\n\n\nZusammenfassung\n\n\n\nZusammenfassend wurden im Zuge der Itemanalyse insgesamt 18 der 29 Items ausgeschlossen. Am Ende dieses Analyseschritts bleiben somit 11 Items Ã¼brig, die fÃ¼r weitere Analysen, insbesondere die explorative Faktorenanalyse (EFA), in Betracht gezogen werden (Details siehe EFA). Die durchschnittliche inter-item-Korrelation von .22 weist darauf hin, dass die Items im Fragebogen in einem moderaten MaÃŸ miteinander korrelieren. Dies deutet darauf hin, dass es eine gewisse gemeinsame Varianz zwischen den Items gibt, ohne dass sie Ã¼bermÃ¤ÃŸig stark miteinander verbunden sind. Ein Cronbachâ€™s \\alpha Wert von \\alpha = .75 lÃ¤sst darauf schlieÃŸen, dass der Fragebogen eine akzeptable ReliabilitÃ¤t aufweist."
  },
  {
    "objectID": "script_itemanalysis.html#zusatz",
    "href": "script_itemanalysis.html#zusatz",
    "title": "1Â  Itemanalyse",
    "section": "1.5 Zusatz",
    "text": "1.5 Zusatz\n\nAlternative ReliabilitÃ¤tsberechnung: McDonaldâ€™s OmegaBerechnung der Itemvarianz\n\n\nAbschlieÃŸend gibt es noch die MÃ¶glichkeit, McDonaldÂ´s \\omega als ein alternatives ReliabilitÃ¤tsmaÃŸ (zusÃ¤tzlich zu CronbachÂ´s \\alpha) zu bestimmen. Wir nutzen dafÃ¼r wieder eine Funktion aus dem psych-Package.\n\nomega_items &lt;- psych::omega(data_item_s2, plot = FALSE)\nomega_items$omega.tot\n\n[1] 0.8306594\n\nomega_items$alpha\n\n[1] 0.7582772\n\n\n\n\nNeben der Berechnung der Itemschwierigkeit und TrennschÃ¤rfe ist es sinnvoll, auch die Itemvarianz zu analysieren. Die Itemvarianz stellt zwar kein Selektionskriterium dar, da sie nicht standardisiert ist, jedoch ist sie wichtig, um festzustellen, ob Personen in ihrem Antwortverhalten bei einem bestimmten Item Ã¼berhaupt variieren (ob die Itemvarianz &gt; 0 ist). Zudem ermÃ¶glicht die Itemvarianz Vergleiche zwischen verschiedenen Items.\n\ndiag(var(data_item_s2))\n\n    ub_06     ub_08     ub_09     ub_10     ub_13   ub_14_r     ub_18     ub_19 \n2.6148515 2.2800000 1.6295050 1.8499010 1.6455446 1.9675248 1.3279208 2.1742574 \n    ub_20     ub_24   ub_27_r \n2.0400000 0.8720792 1.4178218"
  },
  {
    "objectID": "script_itemanalysis.html#ausblick",
    "href": "script_itemanalysis.html#ausblick",
    "title": "1Â  Itemanalyse",
    "section": "1.6 Ausblick",
    "text": "1.6 Ausblick\nIm kommenden Skript setzen wir uns mit der Exploratorischen Faktorenanalyse (EFA) auseinander. Daher ist es sinnvoll, den Zwischenstand bzw. die Ergebnisse der Itemanalyse zu speichern, um nahtlos von diesem Punkt aus fortzufahren. Zu diesem Zweck sichern wir den finale Dataframe data_item_s2 als csv-Datei.\n\nreadr::write_csv(x = data_item_s2, file = here(\"data/processed\", \"data_item_itemanalysis.csv\"))\n# oder write.csv()"
  },
  {
    "objectID": "script_itemanalysis.html#session-info",
    "href": "script_itemanalysis.html#session-info",
    "title": "1Â  Itemanalyse",
    "section": "1.7 Session Info",
    "text": "1.7 Session Info\n\n\n\n\n\n\nErweitern fÃ¼r Session Info\n\n\n\n\n\n\n\nâ”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.1.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2023-11-27\n pandoc   3.1.4 @ /opt/homebrew/bin/ (via rmarkdown)\n\nâ”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n package * version date (UTC) lib source\n dplyr   * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n here    * 1.0.1   2020-12-13 [1] CRAN (R 4.3.0)\n janitor * 2.2.0   2023-02-02 [1] CRAN (R 4.3.0)\n psych   * 2.3.9   2023-09-26 [1] CRAN (R 4.3.1)\n sjmisc  * 2.8.9   2021-12-03 [1] CRAN (R 4.3.0)\n sjPlot  * 2.8.15  2023-08-17 [1] CRAN (R 4.3.0)\n skimr   * 2.1.5   2022-12-23 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  },
  {
    "objectID": "script_efa.html#laden-der-daten-Ã¼bersicht",
    "href": "script_efa.html#laden-der-daten-Ã¼bersicht",
    "title": "2Â  EFA",
    "section": "2.1 Laden der Daten & Ãœbersicht",
    "text": "2.1 Laden der Daten & Ãœbersicht\nWir beginnen mit dem Datensatz, mit dem wir letztes Skript aufgehÃ¶rt haben. Wie immer mÃ¼sst ihr dafÃ¼r den Pfad wÃ¤hlen, in dem sich die Daten befinden. Ihr kÃ¶nnt die finalen Daten aus der Itemeanalyse entweder wieder aus OLAT herunterladen (unter Gemeinsame Dokumente aller Gruppen/R-Skripte_Auswertung) oder direkt den GitHub-Permalink nutzen.\n\nMÃ¶glichkeit I â€“ LokalMÃ¶glichkeit II - Link\n\n\n\nlibrary(here)\ndata_item_final &lt;- read.csv(here(\"data/processed/data_item_itemanalysis.csv\"))\n\n\n\n\nlink &lt;- url(\"https://raw.githubusercontent.com/jlschnatz/PsyBSc8_Diagnostik_WS2324/main/data/processed/data_item_itemanalysis.csv\")\ndata_item_final &lt;- read.csv(link)\n\n\n\n\nMit skim() kÃ¶nnen wir uns nochmal einen kurzen Ãœberblick verschaffen, wie die Datenstruktur aussieht und welche Variablen wir nach der Itemanalyse beibehalten/ausgeschlossen haben.\n\nlibrary(skimr)\nskim(data_item_final)\n\n\nData summary\n\n\nName\ndata_item_final\n\n\nNumber of rows\n101\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n11\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nub_06\n0\n1\n2.69\n1.62\n0\n1\n3\n4\n5\nâ–‡â–ƒâ–†â–…â–…\n\n\nub_08\n0\n1\n3.00\n1.51\n0\n2\n3\n4\n5\nâ–†â–†â–†â–‡â–†\n\n\nub_09\n0\n1\n1.45\n1.28\n0\n1\n1\n2\n5\nâ–‡â–ƒâ–‚â–â–\n\n\nub_10\n0\n1\n2.99\n1.36\n0\n2\n3\n4\n5\nâ–ƒâ–‡â–‡â–‡â–…\n\n\nub_13\n0\n1\n3.66\n1.28\n0\n3\n4\n5\n5\nâ–‚â–‚â–†â–†â–‡\n\n\nub_14_r\n0\n1\n2.95\n1.40\n0\n2\n3\n4\n5\nâ–ƒâ–‡â–‡â–‚â–†\n\n\nub_18\n0\n1\n1.18\n1.15\n0\n0\n1\n2\n4\nâ–‡â–‡â–ƒâ–‚â–\n\n\nub_19\n0\n1\n2.19\n1.47\n0\n1\n2\n3\n5\nâ–‡â–…â–…â–‚â–‚\n\n\nub_20\n0\n1\n2.00\n1.43\n0\n1\n2\n3\n5\nâ–‡â–†â–ƒâ–â–‚\n\n\nub_24\n0\n1\n3.78\n0.93\n0\n3\n4\n4\n5\nâ–â–â–…â–‡â–ƒ\n\n\nub_27_r\n0\n1\n3.73\n1.19\n0\n3\n4\n5\n5\nâ–â–‚â–‡â–‡â–‡"
  },
  {
    "objectID": "script_efa.html#recap-der-itemanalyse",
    "href": "script_efa.html#recap-der-itemanalyse",
    "title": "2Â  EFA",
    "section": "2.2 Recap der Itemanalyse",
    "text": "2.2 Recap der Itemanalyse\nAls Erinnerung kÃ¶nnen wir uns nochmal die psychometrische Eigenschaften der beibehaltenen Items anschauen. Wir benutzen dafÃ¼r wieder die Funktion tab_itemscale() aus dem sjPlot-Package, die wir letztes Skript kennengelernt haben.\n\nlibrary(sjPlot)\ntab_itemscale(df = data_item_final, factor.groups.titles = \"Finale Itemanalyse\")\n\n\nFinale Itemanalyse\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.41\n0.74\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.36\n0.75\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.35\n0.75\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.38\n0.74\n\n\n\nub_13\n0.00 %\n3.66\n1.28\n-0.88\n0.73\n0.35\n0.75\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.55\n0.72\n\n\n\nub_18\n0.00 %\n1.18\n1.15\n0.92\n0.29\n0.23\n0.76\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.51\n0.72\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.43\n0.74\n\n\n\nub_24\n0.00 %\n3.78\n0.93\n-0.83\n0.76\n0.35\n0.75\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.56\n0.72\n\n\n\nMean inter-item-correlation=0.222 Â· Cronbach's Î±=0.758"
  },
  {
    "objectID": "script_efa.html#durchfÃ¼hrung-der-efa",
    "href": "script_efa.html#durchfÃ¼hrung-der-efa",
    "title": "2Â  EFA",
    "section": "2.3 DurchfÃ¼hrung der EFA",
    "text": "2.3 DurchfÃ¼hrung der EFA\n\n2.3.1 Identifikation der Anzahl der Faktoren\nZur Bestimmung der Anzahl der Faktoren in der exploratorischen Faktorenanalyse gibt es mehrere Herangehensweisen, die bereits in dem Lernbar-Video vorgestellt wurden: das Eigenwertkriterium/Kaiser-Guttman-Kriterium [@kaiser1960application], der Scree-Plot [@cattell1966scree] und die Parallelanalyse [@horn1965rationale]. Jedes dieser Verfahren hat seine eigenen Vorbehalte hinsichtlich KonservativitÃ¤t und ObjektivitÃ¤t. Deswegen ist es in der Praxis empfehlenswert, die Ergebnisse mehrerer Kriterien gleichzeitig zu berÃ¼cksichtigen, um die hÃ¶chste Genauigkeit zu erzielen. FÃ¼r eine Ãœbersicht hinsichtlich verschiedener Extraktionsmethoden und deren Vergleich siehe: @auerswald2019determine.\n\n2.3.1.1 MÃ¶glichkeit I - Eigenwertkriterium\nDas Eigenwertkriterium bzw. Kaiser-Kriterium ist das liberalste MaÃŸ der Entscheidung, weswegen tendeziell dadurch viele Faktoren entstehen. Zur Bestimmung werden die Eigenwerte errechnet und alle Faktoren beibehalten, deren Eigenwert \\lambda &gt;= 1 ist. Es gibt zwei MÃ¶glichkeiten, den Eigenwerteverlauf mittels R zu bestimmen.\n\n1. Weg: Base-R Funktion eigen()2. Weg: FactorMineR Package\n\n\nZunÃ¤chst berechnen wir die symmetrische Korrelationsmatrix aus den Daten und kÃ¶nnen dann mit der eigen() Funktion die den Eigenwertverlauf bestimmen. Der Output ist eine Liste, in die wir indizieren mÃ¼ssen, um die Eigenwerte zu extrahieren.\n\n# Base R\neigen(cor(data_item_final))$values \n\n [1] 3.3374011 1.5944637 1.1764625 0.9872941 0.9171743 0.7127462 0.6879672\n [8] 0.6022027 0.4861623 0.3122628 0.1858631\n\nlibrary(tidyverse)\n# Alternative Schreibweise (mit pipes):\ncor(data_item_final) %&gt;% \n  eigen() %&gt;% \n  chuck(\"values\") # chuck() extrahiert einzelne Elemente aus Listen\n\n [1] 3.3374011 1.5944637 1.1764625 0.9872941 0.9171743 0.7127462 0.6879672\n [8] 0.6022027 0.4861623 0.3122628 0.1858631\n\n# in Tabellenform:\ncor(data_item_final) %&gt;% \n  eigen() %&gt;% \n  chuck(\"values\") %&gt;% \n  as_tibble() %&gt;% \n  rename(Eigenvalue = value) %&gt;% \n  tab_df(\n    title = \"Eigenwertverlauf\",\n    col.header = \"Eigenwert\",\n    show.rownames = TRUE\n    )\n\n\nEigenwertverlauf\n\n\nRow\nEigenwert\n\n\n1\n3.34\n\n\n2\n1.59\n\n\n3\n1.18\n\n\n4\n0.99\n\n\n5\n0.92\n\n\n6\n0.71\n\n\n7\n0.69\n\n\n8\n0.60\n\n\n9\n0.49\n\n\n10\n0.31\n\n\n11\n0.19\n\n\n\n\n\n\n\n\n\nMit dem alternativen Weg kÃ¶nnnen wir noch etwas mehr Informationen als nur die Eigenwerte extrahieren. Dazu mÃ¼ssen wir das FactoMineR Package laden bzw. wenn noch nicht geschehen auch installieren. Wir verwenden die PCA() Funktion und extrahieren im Anschluss die Eigenwerte und VarianzaufklÃ¤rung der mÃ¶glichen Faktoren aus der abgespeichertern Liste.\n\n# install.packages(\"FactoMineR\")\nlibrary(FactoMineR)\n\npca &lt;- PCA(data_item_final, graph = FALSE) \n\neigen &lt;- as.data.frame(pca$eig) # als dataframe abspeichern\neigen &lt;- as.data.frame(pca[[\"eig\"]]) # Alternative\n\n# in Tabellenform:\ntab_df(\n  x = eigen,\n  show.rownames = TRUE,\n  title = \"Eigenwertverlauf mit zusÃ¤tzlicher Information hinsichtlich erklÃ¤rte Varianz\",\n  col.header  = c(\"Eigenwert\", \"ErklÃ¤rte Varianz\", \"Kum. erklÃ¤rte Varianz\")\n  ) \n\n\nEigenwertverlauf mit zusÃ¤tzlicher Information hinsichtlich erklÃ¤rte Varianz\n\n\nRow\nEigenwert\nErklÃ¤rte Varianz\nKum. erklÃ¤rte Varianz\n\n\ncomp 1\n3.34\n30.34\n30.34\n\n\ncomp 2\n1.59\n14.50\n44.84\n\n\ncomp 3\n1.18\n10.70\n55.53\n\n\ncomp 4\n0.99\n8.98\n64.51\n\n\ncomp 5\n0.92\n8.34\n72.84\n\n\ncomp 6\n0.71\n6.48\n79.32\n\n\ncomp 7\n0.69\n6.25\n85.58\n\n\ncomp 8\n0.60\n5.47\n91.05\n\n\ncomp 9\n0.49\n4.42\n95.47\n\n\ncomp 10\n0.31\n2.84\n98.31\n\n\ncomp 11\n0.19\n1.69\n100.00\n\n\n\n\n\n\n\n\n\n\nWie anhand der Ergebnisse erkennbar ist, wird durch das Eigenwertkriterium ein Modell mit 3 Faktoren vorgeschlagen.\n\n\n2.3.1.2 MÃ¶glichkeit II - Scree-Plot\nDer Scree-Plot ist ein visuell deskriptives Kriterium zur Entscheidung der Faktoren. Dabei ist das Kriterium konservativer als das Eigenwertkriterium. Wie in dem Lernbar-Video beschrieben wird der optische Knick herangezogen, um die Entscheidung Ã¼ber die Faktorenanzahl zu treffen. Alle Faktoren, die sich â€œÃ¼berâ€ dem Knick befinden, werden beibehalten.\n\n\n\n\n\n\nWelches Package fÃ¼rs Plotting?\n\n\n\nUm in R einen Scree-Plot zu generieren, kÃ¶nnnen wir entweder die Base-R Plotting Funktionen verwenden oder das sehr erfolgreiche ggplot2-Package der tidyverse Familie benutzen. Dabei kÃ¶nnen zwar mit Base R Plotting Funktionen zwar in nur wenigen Zeilen ein Plot erstellt werden, bieten dafÃ¼r aber nicht so viele AnpassungsmÃ¶glichkeiten und folgen keiner klaren Struktur im Vergleich zu ggplot2. Wem es also wichtig sein sollte, schÃ¶ne Plots zu generieren, sollte eher das ggplot2-Package nutzen. Wer nochmal eine ggplot2 Auffrischung brauchen sollte, findet hier ein paar nÃ¼tzliche Links: PandaR, R for Data Science & ggplot2-Buch.\n\n\n\n1. Weg: Base-R2. Weg: ggplot2 Package\n\n\nWir benutzen das psych Package, um den Scree-Plot zu erstellen. Dabei verwendet dieses Package im Hintergrund Base R fÃ¼r das Plotting.\n\nlibrary(psych)\nscree(\n  rx = data_item_final, \n  factors = TRUE,\n  pc = FALSE, # sollen Hauptkomponenten auch dargestellt werden?\n  hline = 1 # Plot mit Knicklinie, hline = -1 -&gt; ohne Linie\n  ) \n\n\n\n\n\n\n\n\n\n\nWir erstellen zunÃ¤chst einen Dataframe, indem die Eigenwerte und Nummerierung der Faktoren als zwei Variablen gespeichert sind. Danach werden diese Daten in Layern mittels ggplot2 geplottet.\n\neigen_res &lt;- eigen(cor(data_item_final))\ndata_eigen &lt;- data.frame(\n  Eigenwert = eigen_res$values,\n  Faktor = seq_along(eigen_res$values) # alternativ 1:length(eigen_res$values)\n)\n\n# Basis-ggplot Layer\nggplot(\n  data = data_eigen, \n  mapping = aes(x = Faktor, y = Eigenwert) # Zuordnung von Variablen zu Koordinatenachsen\n  ) + \n  # Horizontale Linie bei y = 1\n   geom_hline(\n    color = \"darkgrey\", \n    yintercept = 1,\n    linetype = \"longdash\" \n    ) +\n  # HinzufÃ¼gen der Linien\n  geom_line(alpha = 0.6, color = \"royalblue4\") + \n  # HinzufÃ¼gen der Punkte\n  geom_point(size = 3, color = \"royalblue4\") +\n  # AchsenverÃ¤nderung der y-Achse\n  scale_y_continuous(\n    breaks = seq(0, 4, 1),\n    limits = c(0, 4),\n    expand = c(0, 0)\n  ) +\n  # AchsenverÃ¤nderung der x-Achse\n  scale_x_continuous(\n    breaks = seq(1, 11),\n    limits = c(1, 11)\n  ) +\n  # Theme\n  theme_sjplot() \n\n\n\n\n\n\n\n\n\n\n\nDie Ergebnisse des Scree-Plots sind in diesem Fall nicht ganz eindeutig interpretierbar, da kein klarer Knick im Verlauf erkennbar ist. Hier wird auch ein Nachteil dieser Methode deutlich: die visuelle Interpretation bleibt immer bis zu einem gewissen Grad subjektiv. Am ehesten scheint ein Modell mit zwei Faktoren gemÃ¤ÃŸ dem Scree-Plot am sinnvollsten. Daher wÃ¤re die Entscheidung gemÃ¤ÃŸ dem Scree-Plot im Vergleich zum Kaiser-Kriterium konservativer.\n\n\n2.3.1.3 MÃ¶glichkeit III - Parallelanalyse\nDie letzte vorgestellte MÃ¶glichkeit ist die Parallelanalyse. Dabei werden die Eigenwerte der Faktorenanalyse des realen Datensatzes mit denen eines Datensatzes mit normalverteilten unkorrelierten Zufallsdaten verglichen, wobei Faktoren beibehalten werden, deren Eigenwerte grÃ¶ÃŸer sind als die gemittelten Eigenwerte der Zufallsdaten [@Klopp_2010]. Anders formuliert werden Faktoren beibehalten, die mehr Varianz aufklÃ¤ren als rein zufÃ¤llig simulierte Daten. Von den drei vorgestellten Kriterien ist diese das konservativste Kriterium.\n\nMÃ¶glichkeit I - psych-Package und base RMÃ¶glichkeit II - ggplot2\n\n\n\npsych::fa.parallel(\n  x = data_item_final, \n  fm = \"pa\", # Principal Axis Factoring Extraktion\n  fa = \"fa\", # Factor Analysis (fa = \"pc\" fÃ¼r Hauptkomponentenanalyse)\n  n.iter = 1000, # Anzahl der Simulationen\n  quant = .95, # Vergleichsintervall\n  main = \"Parallelanalyse mittels Base R und psych-Package\",\n  ylabel = \"Eigenwert\",\n  error.bars = FALSE # TRUE fÃ¼r Error-Bars\n  )\n\n\n\n\n\n\n\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n\n\n\nWir speichern zunÃ¤chst die ausgegeben Daten der Funktion fa.parallel() in einem Objekt ab. Danach erstellen wir einen Dateframe mit den relevanten Informationen (empirische und simulierte Eigenwerte).\n\ndata_pa &lt;- fa.parallel(\n  x = data_item_final, \n  fm= \"pa\", \n  fa = \"fa\", \n  plot = FALSE,\n  n.iter = 1000,\n  quant = .95\n  )\n\nParallel analysis suggests that the number of factors =  2  and the number of components =  NA \n\n# Dataframe\ndata_plot &lt;- data.frame(\n  Observed = data_pa$fa.values, # empirisch \n  Simulated = data_pa$fa.sim    # simuliert\n  )\n\n# Zeilennamen als eigene Variable \"Factor\"\ndata_plot$Factor &lt;- as.integer(rownames(data_plot))\n\ntab_df(data_plot)\n\n\n\n\nObserved\nSimulated\nFactor\n\n\n2.67\n0.80\n1\n\n\n0.88\n0.44\n2\n\n\n0.30\n0.31\n3\n\n\n0.22\n0.21\n4\n\n\n0.12\n0.11\n5\n\n\n-0.07\n0.03\n6\n\n\n-0.12\n-0.05\n7\n\n\n-0.23\n-0.13\n8\n\n\n-0.33\n-0.21\n9\n\n\n-0.36\n-0.30\n10\n\n\n-0.41\n-0.40\n11\n\n\n\n\n\n\n\n\nUm Daten mit ggplot2 zu visualisieren, mÃ¼ssen sie im Long-Format vorliegen. In diesem Kontext bedeutet das, dass jede Beobachtung durch drei Variablen reprÃ¤sentiert wird: eine fÃ¼r den Faktor, eine zur Kennzeichnung, ob der Datenpunkt simuliert oder empirisch ist, und eine fÃ¼r die Eigenwerte.\nDie praktische Funktion pivot_longer() aus dem tidyr-Package ermÃ¶glicht die Transformation von Dataframes vom Wide-Format ins Long-Format (analog dazu kann pivot_wider() von Wide zu Long genutzt werden). Dabei kÃ¶nnen wir mit dem cols-Argument die zu transformierenden Variablen spezifizieren, und mit names_to und values_to geben wir den neu erstellten Variablen eigene Namen.\n\ndata_plot_long &lt;- pivot_longer(\n  data = data_plot,\n  cols = c(Observed, Simulated),\n  names_to = \"Typ\",\n  values_to = \"Eigenwert\"\n  )\n\ntab_df(data_plot_long)\n\n\n\n\nFactor\nTyp\nEigenwert\n\n\n1\nObserved\n2.67\n\n\n1\nSimulated\n0.80\n\n\n2\nObserved\n0.88\n\n\n2\nSimulated\n0.44\n\n\n3\nObserved\n0.30\n\n\n3\nSimulated\n0.31\n\n\n4\nObserved\n0.22\n\n\n4\nSimulated\n0.21\n\n\n5\nObserved\n0.12\n\n\n5\nSimulated\n0.11\n\n\n6\nObserved\n-0.07\n\n\n6\nSimulated\n0.03\n\n\n7\nObserved\n-0.12\n\n\n7\nSimulated\n-0.05\n\n\n8\nObserved\n-0.23\n\n\n8\nSimulated\n-0.13\n\n\n9\nObserved\n-0.33\n\n\n9\nSimulated\n-0.21\n\n\n10\nObserved\n-0.36\n\n\n10\nSimulated\n-0.30\n\n\n11\nObserved\n-0.41\n\n\n11\nSimulated\n-0.40\n\n\n\n\n\n\n\nEine alternative Methode, die ausschlieÃŸlich auf base-R basiert, wird hier beschrieben.\n\ndata_plot_long &lt;- reshape(\n    data = data_plot,\n    varying = c(\"Observed\", \"Simulated\"), \n    v.names = \"Eigenwert\",\n    timevar = \"Typ\",\n    times = c(\"Observed\", \"Simulated\"),\n    direction = \"long\"\n  )\n\nJetzt kÃ¶nnen wir die Abbildung mit ggplot2 generieren. Dabei definieren wir neben x und y mit aes(color = Typ), dass diese Variable als Farbkodierung dargestellt werden soll.\n\nggplot(\n    # Daten\n    data = data_plot_long,\n    # Aesthetics\n    aes(x = Factor, y = Eigenwert, color = Typ)\n    ) + \n  # Linien\n  geom_line(\n    size = 0.7,\n    alpha = .8\n    ) + \n  # Punkte\n  geom_point(\n    size = 3.5,\n    alpha = .8\n    ) + \n  # AchsenverÃ¤nderungen\n  scale_y_continuous(\n    breaks = seq(-1, 3, 1),\n    limits = c(-1, 3),\n    expand = c(0, 0)\n  ) +\n  scale_x_continuous(\n    breaks = seq(1, 11),\n    limits = c(1, 11)\n  ) + \n  # Farben verÃ¤ndern\n  scale_color_manual(values = c(\"royalblue4\", \"orangered2\")) +\n  # Themes\n  theme_sjplot() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nDie Ergebnisse der Parallelanalyse suggerieren ein Modell mit zwei Faktoren. Da dies das konservativste Vorgehen ist, entscheiden wir uns fÃ¼r ein zweifaktorielle Modell fÃ¼r die weitere Analyse.\n\n\n\n2.3.2 Extraktion der Faktoren & Rotation\nAls nÃ¤chsten Schritt kÃ¶nnen wir nun die eigentliche exploratorische Faktoranalyse durchfÃ¼hren. Wir benutzen dafÃ¼r wieder ein Befehl aus dem sjPlot-Package, mit dem wir direkt die Faktorstruktur als Tabelle darstellen kÃ¶nnen. Es gibt aber auch viele alternative Packages, mit denen eine EFA durchgefÃ¼hrt werden kann (z.B. psych fa() , mit dem parameters-Paket factor_analysis() oder base-R factanal()).\nWir spezifizieren ein Modell mit 2 Faktoren, die mittels Principal Axis Factoring extrahiert werden (method = \"pa\") und fÃ¼hren anschlieÃŸend eine oblimine Rotation durch (rotation = \"oblimin\"). Wie in der Lernbar bereits vorgestellt, gibt es unterschiedliche Rotationsverfahren nach der Extraktion der Faktoren. Bei orthogonalen Rotation bleiben die latenten Faktoren unkorreliert, wohingegen bei obliquer Rotation die Faktoren miteinander korrelieren dÃ¼rfen. Alle Rotationsverfahren haben das Ziel, mÃ¶glichst eine Einfachstruktur zu erhalten, d.h. dass jedes Item mÃ¶glichst nur auf einen Faktor lÃ¤dt. FÃ¼r einen Vergleich verschiedener Rotationsverfahren und wann welches angewendet werden sollte: @costello2005best.\n\nfit_fa &lt;- tab_fa(\n  data = data_item_final,\n  nmbr.fctr = 2, # Faktorenanzahl\n  rotation = \"oblimin\", # Rotationsverfahren\n  fctr.load.tlrn = 0, \n  method = \"pa\", # Alternative Methoden aus Lernbar, \"ml\" fÃ¼r Maximum-Likelihood \n  title = \"Faktorenanalyse\",\n  #file = \"fit_fa.doc\" # Ergebnisse kÃ¶nnen wieder als .doc gespeichert werden\n  )\n\nfit_fa\n\n\nFaktorenanalyse\n\n\nÂ \nFactor 1\nFactor 2\n\n\nub_06\n-0.07\n0.69\n\n\nub_08\n0.01\n0.47\n\n\nub_09\n-0.09\n0.59\n\n\nub_10\n0.13\n0.37\n\n\nub_13\n0.26\n0.21\n\n\nub_14_r\n0.99\n-0.04\n\n\nub_18\n0.14\n0.16\n\n\nub_19\n0.19\n0.54\n\n\nub_20\n0.73\n-0.03\n\n\nub_24\n0.20\n0.29\n\n\nub_27_r\n0.62\n0.24\n\n\nCronbach's Î±\n0.76\n0.67\n\n\n\n\n\nDer Output besteht erneut aus einer HTML-Tabelle, die die Ladungen der 11 Items auf beide Faktoren sowie die ReliabilitÃ¤t der beiden Skalen anzeigt. Die jeweils hÃ¶here Ladung ist dabei hervorgehoben. Wir erinnern uns daran, dass bei der EFA es zwei Selektionskriterien zur Ermittlung der bestmÃ¶glichen Einfachstruktur gibt: hohe Faktorladungen (Auschluss von Item \\lambda &lt; .3 bzw. konservativer \\lambda &lt; .4) und Ausschluss von Items mit Doppelladungen (Cutoff z.B. bei \\mid~ \\lambda_1 - \\lambda_2 \\mid &lt; .1). Um erneut programmatisch die Items zu ermitteln, die anhand dieser Cutoff-Kriterium ausgeschlossen werden sollten, fÃ¼hren wir nochmal die Faktorenanalyse mit dem psych-Paket durch. Der Grund dafÃ¼r liegt darin, dass das psych-Paket als Output der Funktion die Faktorladungen ausgibt, auf die wir mithilfe der loadings() Funktion zugreifen kÃ¶nnen. Danach kÃ¶nnen wir mit ein bisschen Logik mit der Funktion filter() aus dem dplyr-Paket alle Items filtern, die ausgeschlossen werden sollten.\n\nres_psych &lt;- psych::fa(\n  r = data_item_final, \n  nfactors = 2, # 2 Faktoren\n  fm = \"pa\" # Principal Axis Factoring\n  ) \n\ndf_fa &lt;- as.data.frame.matrix(loadings(res_psych))\ndf_fa$id_item &lt;- rownames(df_fa) # Neue Variable mit den Itemnamen\n\nstep_kick3_efa &lt;- df_fa %&gt;%\n  filter(\n    # 1. Kriterium\n    (PA1 &lt; .3 & PA2 &lt; .3) # Ladungen sowohl beim ersten als auch zweiten Faktor &lt;.3\n    | # ODER\n    # 2. Kriterium\n    (abs(PA1 - PA2) &lt; .1) # Absolute Differenz beider Faktoren &lt; .1\n    ) %&gt;%\n  pull(id_item)\n\n\n\n2.3.3 Finale Modellspezifizierung\nDie Faktorladungen von Item 13, Item 18 und Item 24 liegen unterhalb des festgelegten Cut-off-Kriteriums und wurden daher von der Filterfunktion als ausschlusswÃ¼rdig identifiziert. Alle anderen Items laden weitestgehend gut auf einen Faktor und besitzen mittlere bis sehr hohe Faktorladungen. FÃ¼r eine abschlieÃŸende Modellspezifizierung erstellen wir einen neuen Datensatz ohne die drei genannten Variablen und fÃ¼hren erneut eine Faktorenanalyse durch.\n\ndata_item_final_s3_efa  &lt;- select(data_item_final, -all_of(step_kick3_efa))\n\ntab_fa(\n  data = data_item_final_s3_efa, \n  nmbr.fctr = 2,\n  rotation = \"oblimin\", \n  fctr.load.tlrn = 0, \n  method = \"pa\",\n  title = \"Finale Faktorenanalyse\"\n  )\n\n\nFinale Faktorenanalyse\n\n\nÂ \nFactor 1\nFactor 2\n\n\nub_06\n-0.05\n0.72\n\n\nub_08\n0.02\n0.45\n\n\nub_09\n-0.07\n0.61\n\n\nub_10\n0.13\n0.33\n\n\nub_14_r\n0.99\n-0.04\n\n\nub_19\n0.21\n0.52\n\n\nub_20\n0.74\n-0.03\n\n\nub_27_r\n0.64\n0.23\n\n\nCronbach's Î±\n0.83\n0.67"
  },
  {
    "objectID": "script_efa.html#deskriptive-analyse-der-faktorstruktur",
    "href": "script_efa.html#deskriptive-analyse-der-faktorstruktur",
    "title": "2Â  EFA",
    "section": "2.4 Deskriptive Analyse der Faktorstruktur",
    "text": "2.4 Deskriptive Analyse der Faktorstruktur\nAnschlieÃŸend kÃ¶nnen wir fÃ¼r jeden Faktor getrennt eine finale Itemanalyse durchfÃ¼hren. Das sjPlot Paket kann dabei alle zwei Tabellen fÃ¼r die Faktoren gleichzeitig in einer Funktion berechnen. DafÃ¼r muss zunÃ¤chst jedoch ein Index erstellt werden, der spezifiziert, welche Items zu welchem Faktor gehÃ¶ren. Danach kann durch das Argument factor.groups die Faktorstruktur spezifiziert werden.\n\nfa_index &lt;- tab_fa(\n  data = data_item_final_s3_efa, \n  nmbr.fctr = 2, \n  rotation = \"oblimin\", \n  fctr.load.tlrn = 0, \n  method = \"pa\"\n  )$factor.index\n\nprint(fa_index)\n\n  ub_06   ub_08   ub_09   ub_10 ub_14_r   ub_19   ub_20 ub_27_r \n      2       2       2       2       1       2       1       1 \n\ntab_itemscale(\n  df = data_item_final_s3_efa, \n  factor.groups = fa_index, \n  factor.groups.titles = c(\"Faktor 1\", \"Faktor 2\") \n  )\n\n\nFaktor 1\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nub_14_r\n0.00 %\n2.95\n1.4\n0.05\n0.59\n0.80\n0.65\n\n\n\nub_20\n0.00 %\n2\n1.43\n0.44\n0.40\n0.68\n0.79\n\n\n\nub_27_r\n0.00 %\n3.73\n1.19\n-0.99\n0.75\n0.62\n0.84\n\n\n\nMean inter-item-correlation=0.626 Â· Cronbach's Î±=0.834\n\n\n\nÂ \n\nFaktor 2\n\n\nRow\nMissings\nMean\nSD\nSkew\nItem Difficulty\nItem Discrimination\nÎ± if deleted\n\n\n\nub_06\n0.00 %\n2.69\n1.62\n-0.11\n0.54\n0.52\n0.57\n\n\n\nub_08\n0.00 %\n3\n1.51\n-0.43\n0.60\n0.40\n0.63\n\n\n\nub_09\n0.00 %\n1.45\n1.28\n0.96\n0.29\n0.47\n0.60\n\n\n\nub_10\n0.00 %\n2.99\n1.36\n-0.35\n0.60\n0.30\n0.67\n\n\n\nub_19\n0.00 %\n2.19\n1.47\n0.24\n0.44\n0.44\n0.61\n\n\n\nMean inter-item-correlation=0.288 Â· Cronbach's Î±=0.670\n\n\n\nÂ \n\n\n\n\n\n\n\n\nÂ \nComponent 1\nComponent 2\n\n\nComponent 1\nÎ±=0.834\nÂ \n\n\nComponent 2\n0.313\n(.001)\nÎ±=0.670\n\n\nComputed correlation used pearson-method with listwise-deletion.\n\n\n\n\n\nEbenfalls sollte zusÃ¤tzlich zur internen Konsistenz CronbachÂ´s \\alpha auch McDonaldÂ´s \\omega fÃ¼r jede Skala berechnet werden. DafÃ¼r erstellen wir zwei Variablen, die die Itemnamen fÃ¼r die beiden Skalen enthalten. Nun kÃ¶nnen wir eine kleine Funktion erstellen, mit der wir immer wieder fÃ¼r einen Datensatz \\alpha und \\omega berechnen kÃ¶nnen. Wir sparen uns somit die Arbeit, jedes Mal wieder den Code zu wiederholen, welcher im Body (hinter der eckigen Klammern) der Funktion steht. Wichtig ist, dass wir mit der return() angeben, was letztendlich dem User ausgegeben werden soll (sonst wird nichts angezeigt).\n\nf1_names &lt;- names(fa_index[fa_index == 1])\nf2_names &lt;- names(fa_index[fa_index == 2])\n\n\nget_reliability &lt;- function(.data, .var_names) {\n  fit_omega &lt;- omega(.data[, .var_names], plot = FALSE)\n  out &lt;- list(\n    omega = fit_omega$omega.tot, \n    alpha = fit_omega$alpha\n  )\n  # Angabe, welche Objekte ausgegeben werden sollen\n  return(out)\n}\n \nget_reliability(data_item_final_s3_efa, f1_names)\n\n$omega\n[1] 0.8538191\n\n$alpha\n[1] 0.8341717\n\nget_reliability(data_item_final_s3_efa, f2_names)\n\n$omega\n[1] 0.7115663\n\n$alpha\n[1] 0.6690035"
  },
  {
    "objectID": "script_efa.html#testwertanalyse",
    "href": "script_efa.html#testwertanalyse",
    "title": "2Â  EFA",
    "section": "2.5 Testwertanalyse",
    "text": "2.5 Testwertanalyse\nAls letztes werden wir die deskriptiven Kennwerte der Testwerte der drei Skalen genauer betrachten. Diese mÃ¼ssen wir zunÃ¤chst erstmal berechnen. DafÃ¼r gibt es mehrere MÃ¶glichkeiten:\n\nMÃ¶glichkeit I - base RMÃ¶glichkeit II - tidyverse\n\n\n\ndata_item_final_scores &lt;- data_item_final_s3_efa\ndata_item_final_scores$f1 &lt;- rowSums(data_item_final_scores[, f1_names])\ndata_item_final_scores$f2 &lt;- rowSums(data_item_final_scores[, f2_names])\ndata_item_final_scores$ub_score &lt;- rowSums(data_item_final_scores)\n\ncolnames(data_item_final_scores)\n\n [1] \"ub_06\"    \"ub_08\"    \"ub_09\"    \"ub_10\"    \"ub_14_r\"  \"ub_19\"   \n [7] \"ub_20\"    \"ub_27_r\"  \"f1\"       \"f2\"       \"ub_score\"\n\n\n\n\nÃœber die Funktion rowwise() wird angegeben, dass die nachfolgenden Operationen Ã¼ber die Zeilen (und nicht Ã¼ber die Spalten wie Ã¼blich) hinweg stattfinden sollen. Die Items, die aufsummiert werden, kÃ¶nnen Ã¼ber die Funktion c_across(variablen) angegeben werden.\n\ndata_item_final_scores &lt;- data_item_final_s3_efa %&gt;% \n  rowwise() %&gt;% \n  mutate(ub_score = sum(c_across(everything()))) %&gt;% \n  mutate(\n    f1 = sum(c_across(all_of(f1_names))),\n    f2 = sum(c_across(all_of(f2_names)))\n    ) \n\ncolnames(data_item_final_scores)\n\n [1] \"ub_06\"    \"ub_08\"    \"ub_09\"    \"ub_10\"    \"ub_14_r\"  \"ub_19\"   \n [7] \"ub_20\"    \"ub_27_r\"  \"ub_score\" \"f1\"       \"f2\"      \n\n\n\n\n\nWir kÃ¶nnen uns mittels der describe() des psych-Pakets, welche bereits im letzten Skript vorgestellt wurde die wichtigsten deskriptiven Kennwerte ausgeben lassen.\n\ntab_df(describe(data_item_final_scores$f1))\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n8.68\n3.50\n8\n8.63\n2.97\n0\n15\n15\n0.12\n-0.53\n0.35\n\n\n\n\n\n\ntab_df(describe(data_item_final_scores$f2))\n\n\n\n\nvars\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nskew\nkurtosis\nse\n\n\n1\n101\n12.32\n4.77\n13\n12.40\n4.45\n1\n24\n23\n-0.15\n-0.31\n0.47\n\n\n\n\n\n\n\nDiese deskriptiven Kennwerte kÃ¶nnen wir zudem abschlieÃŸend in einem Histogramm mit unterlegter Normalverteilung visualisieren. Wir benutzen fÃ¼r das Plotten der Normalverteilung das Package ggh4x mit der Funktion stat_theodensity(). Das praktische hierbei ist, dass wir nicht aktiv die Parameter der Normalverteilung (M und SD) angeben mÃ¼ssen, sondern die Funktion das fÃ¼r uns im Hintergrund auf Basis der Daten berechnet. Im nachfolgenden Beispiel wird der Plot fÃ¼r den Gesamttestwert (alle Items) berechnet. Wenn eine der 3 Skalen geplottet werden soll, muss dies nur in der ggplot() Funktion angegeben werden (s.h. Code).\n\nlibrary(ggh4x)\n\nggplot(data_item_final_scores, aes(x = ub_score)) +  # oder x = f1/f2\n  geom_histogram(\n    binwidth = 3,\n    fill = \"lightgrey\",\n    ) +\n  stat_theodensity(\n    mapping = aes(y = after_stat(count)), \n    distri = \"norm\", \n    color = \"royalblue4\",\n    size = 0.5\n    ) +\n  scale_x_continuous(\n    name = \"Umweltbewusstsein\",\n    limits = c(0, 40), # Achsenlimit\n    breaks = seq(0, 40, 5), # Achsenabschnitte\n    expand = c(0, 0)\n    ) +\n  scale_y_continuous(\n    name = \"HÃ¤ufigkeit\",\n    limits = c(0, 20),\n    breaks = seq(0, 20, 5),\n    expand = c(0, 0)\n  ) +\n  theme_sjplot()\n\n\n\n\n\n\n\n\nFÃ¼r diejenigen unter euch, die sich in etwas anspruchvolleres R bzw. ggplot2 Territorium wagen wollen: Es gibt auch die MÃ¶glichkeit, alle Skalen gleichzeitig in einer Abbildung darzustellen. Um jeden einzelnen Schritt der groÃŸen Pipe zu verstehen, geht am besten jeden einzelnen Layer durch und schaut, wie sich der Plot verÃ¤ndert. Auch kann es hilfreich sein, wenn man die einzelnen Parameter in den Layern verÃ¤ndert, um zu schauen, was diese bewirken. Es reicht allerdings natÃ¼rlich auch vÃ¶llig aus, wenn ihr euch an dem letzten Plot orientiert!\n\n\nShow the code\nlibrary(ggh4x)\nlibrary(jcolors)\n\nstrip_labels &lt;- as_labeller(c(f1 = \"Factor 1\", f2 = \"Factor 2\", ub_score = \"Gesamtscore\"))\n\ndata_item_final_scores %&gt;% \n  # Variablen auswÃ¤hlen, die benÃ¶tigt werden\n  select(f1, f2, ub_score) %&gt;% \n  # Vom Wide ins Long-Format\n  pivot_longer(\n    cols = everything(),\n    names_to = \"factor\",\n    values_to = \"score\"\n    ) %&gt;% \n  # ggplot Base Layer\n  ggplot(aes(x = score, fill = factor)) + \n  # facet_wrap erstellt kleineres Plotting Fenster anhand einer Variable\n  facet_wrap(\n    facets  = vars(factor), # Variable, welche gefacetted werden soll\n    labeller = strip_labels, # Facet Labels\n    scales = \"free\"  # freies Koordinatensystem\n    )  +\n  # Histogramm\n  geom_histogram(\n    bins = 15,\n    alpha = 0.6,\n    show.legend = FALSE,\n    aes(fill = factor)\n  ) +\n  # Theoretische Normalverteilung\n  stat_theodensity(aes(y = after_stat(count), color = factor)) +\n  # Achsenmanipulation in den einzelnen Facets\n  facetted_pos_scales(\n    x = list(\n      factor == \"f1\" ~ scale_x_continuous(limits = c(0, 25)),\n      factor == \"f2\" ~ scale_x_continuous(limits = c(0, 25)),\n      factor == \"ub_score\" ~ scale_x_continuous(limits = c(0, 40))\n      )\n    ) +\n  # Achsenmanipulation & Beschriftung\n  scale_y_continuous(\n    expand = c(0, 0),\n    limits = c(0, 40),\n    breaks = seq(0, 40, 10)\n    ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  xlab(\"Score\") + \n  ylab(\"HÃ¤ufigkeit\") +\n  # Farben\n  scale_fill_manual(values = c(\"royalblue4\", \"orangered2\", \"ivory4\")) +\n  scale_color_manual(values = c(\"royalblue4\", \"orangered2\", \"ivory4\")) +\n  guides(color = \"none\", fill = \"none\") +\n  # Theme\n  theme_sjplot()"
  },
  {
    "objectID": "script_efa.html#session-info",
    "href": "script_efa.html#session-info",
    "title": "2Â  EFA",
    "section": "2.6 Session Info",
    "text": "2.6 Session Info\n\n\n\n\n\n\nErweitern fÃ¼r Session Info\n\n\n\n\n\n\n\nâ”€ Session info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       macOS Sonoma 14.1.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Berlin\n date     2023-11-27\n pandoc   3.1.4 @ /opt/homebrew/bin/ (via rmarkdown)\n\nâ”€ Packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n package    * version date (UTC) lib source\n dplyr      * 1.1.3   2023-09-03 [1] CRAN (R 4.3.0)\n FactoMineR * 2.9     2023-10-12 [1] CRAN (R 4.3.1)\n forcats    * 1.0.0   2023-01-29 [1] CRAN (R 4.3.0)\n ggh4x      * 0.2.6   2023-08-30 [1] CRAN (R 4.3.0)\n ggplot2    * 3.4.4   2023-10-12 [1] CRAN (R 4.3.1)\n here       * 1.0.1   2020-12-13 [1] CRAN (R 4.3.0)\n jcolors    * 0.0.5   2023-09-26 [1] Github (jaredhuling/jcolors@81e72c8)\n lubridate  * 1.9.2   2023-02-10 [1] CRAN (R 4.3.0)\n psych      * 2.3.9   2023-09-26 [1] CRAN (R 4.3.1)\n purrr      * 1.0.2   2023-08-10 [1] CRAN (R 4.3.0)\n readr      * 2.1.4   2023-02-10 [1] CRAN (R 4.3.0)\n sjPlot     * 2.8.15  2023-08-17 [1] CRAN (R 4.3.0)\n skimr      * 2.1.5   2022-12-23 [1] CRAN (R 4.3.0)\n stringr    * 1.5.0   2022-12-02 [1] CRAN (R 4.3.0)\n tibble     * 3.2.1   2023-03-20 [1] CRAN (R 4.3.0)\n tidyr      * 1.3.0   2023-01-24 [1] CRAN (R 4.3.0)\n tidyverse  * 2.0.0   2023-02-22 [1] CRAN (R 4.3.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
  }
]